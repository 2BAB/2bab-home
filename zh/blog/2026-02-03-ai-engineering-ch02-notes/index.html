<html lang="zh" dir="ltr">

<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta property="og:title" content="《AI Engineering》笔记 CH02 Foundation Models" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://2bab.me/zh/blog/2026-02-03-ai-engineering-ch02-notes/" />
  <link rel="stylesheet" href="/styles/main.css">
  <link rel="me" href="https://2bab.me/">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@1.27.0/themes/prism.min.css">
  
  <title>《AI Engineering》笔记 CH02 Foundation Models | 2BAB&#39;s Blog</title>
  
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NFRNXW3SHS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-NFRNXW3SHS');
</script>

<body>
  
<div>
  <div class="container mx-auto prose py-12 sm:py-24 px-12 sm:px-0">
    <div class="mb-12">
      <a class="no-underline font-bold" href="/zh">2BAB&#39;s Blog</a>
    </div>
    <h1>《AI Engineering》笔记 CH02 Foundation Models</h1>
    <div class="italic text-gray-500">
      2026/02/03
    </div>
    <div>
      <h2 id="seq2seq-vs-transformer" tabindex="-1">Seq2Seq vs Transformer</h2>
<p>这段文本和图片主要讲了两个核心概念：<strong>模型部署的权衡</strong> 和 <strong>两种核心架构的对比（Seq2Seq vs Transformer）</strong>。</p>
<p>作为一个 Android 工程师，你可以把这段内容理解为**“自然语言处理（NLP）领域的架构演进史”**，就像 Android 开发从早期的 <code>MVC</code> 演进到 <code>MVP</code> 再到现在的 <code>MVVM/MVI</code> 一样。</p>
<p>我用 Android 开发的术语来帮你拆解一下：</p>
<h3 id="1.-%E9%80%89%E5%9E%8B%E4%B8%8E%E6%80%A7%E8%83%BD%E5%BC%80%E9%94%80%EF%BC%88%E5%B7%A6%E4%B8%8A%E8%A7%92%E6%96%87%E6%9C%AC%EF%BC%89" tabindex="-1">1. 选型与性能开销（左上角文本）</h3>
<p>文本第一段在讨论**“Build Config”**的问题。</p>
<ul>
<li><strong>原文意思</strong>：在训练模型前，开发者要决定模型长什么样（架构）以及有多大（参数量）。</li>
<li><strong>Android 类比</strong>：
<ul>
<li><strong>参数量 (Parameters)</strong>：这就好比你的 <strong>APK 体积</strong> 和 <strong>运行时内存占用</strong>。</li>
<li><strong>7B (70亿) 参数模型</strong>：像是一个经过 ProGuard 混淆优化过的轻量级库，勉强可以在高端 Android 手机的 NPU 上跑（端侧部署）。</li>
<li><strong>175B (1750亿) 参数模型</strong>：像是一个巨大的服务端应用，根本不可能塞进手机里，必须通过 API 调云端接口。</li>
<li><strong>Latency (延迟)</strong>：优化 Transformer 就像优化 RecyclerView 的滑动流畅度，不同的架构决定了响应速度。</li>
</ul>
</li>
</ul>
<h3 id="2.-%E6%97%A7%E6%9E%B6%E6%9E%84%EF%BC%9Aseq2seq-(rnn-based)-%E2%80%94%E2%80%94-%22%E5%8D%95%E7%BA%BF%E7%A8%8B%E6%B5%81%E5%BC%8F%E5%A4%84%E7%90%86%22" tabindex="-1">2. 旧架构：Seq2Seq (RNN-based) —— &quot;单线程流式处理&quot;</h3>
<p>看图中的<strong>上半部分 (Seq2Seq)</strong>。这是 2014-2017 年的主流技术（Google 翻译早期就用这个）。</p>
<ul>
<li><strong>工作原理</strong>：
<ol>
<li><strong>Encoder (编码器)</strong>：像一个 <code>InputStream</code>，<strong>串行</strong>读取输入（&quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;?&quot;）。它必须按顺序读，读完一个词，更新一下内部状态。</li>
<li><strong>Final hidden state (中间那个蓝框)</strong>：这是关键瓶颈！它相当于把整个句子的含义压缩成了一个<strong>单一的 <code>Context</code> 对象</strong>或者一个 <code>Bundle</code>。</li>
<li><strong>Decoder (解码器)</strong>：拿到这个 <code>Bundle</code>，开始生成翻译结果。</li>
</ol>
</li>
<li><strong>Android 类比</strong>：
<ul>
<li>想象你有一个巨大的 JSON 文件。这个架构要求你必须从头读到尾，然后把整个文件的内容<strong>死记硬背</strong>存到一个 <code>String</code> 变量里。</li>
<li>然后，你把这个 <code>String</code> 传给下一个 Activity 去处理。</li>
</ul>
</li>
<li><strong>缺点</strong>：如果句子很长（JSON 很大），那个单一的 <code>String</code> 变量存不下那么多细节，前面的内容容易被“遗忘”。这就是所谓的<strong>长距离依赖问题</strong>。</li>
</ul>
<h3 id="3.-%E6%96%B0%E6%9E%B6%E6%9E%84%EF%BC%9Atransformer-%E2%80%94%E2%80%94-%22%E8%A7%82%E5%AF%9F%E8%80%85%E6%A8%A1%E5%BC%8F-%2B-%E9%9A%8F%E6%9C%BA%E5%AD%98%E5%8F%96%22" tabindex="-1">3. 新架构：Transformer —— &quot;观察者模式 + 随机存取&quot;</h3>
<p>看图中的<strong>下半部分 (Transformer)</strong>。这是现在的王者（ChatGPT、DeepSeek 都是基于这个）。</p>
<ul>
<li><strong>工作原理</strong>：
<ul>
<li>注意那些<strong>虚线箭头</strong>。</li>
<li>当模型要生成翻译结果 &quot;Como&quot; (How) 时，它不仅仅依赖上一个词，而是<strong>直接“看”向原始输入的所有单词</strong>。</li>
<li>它会计算一个权重（Attention）：生成 &quot;Como&quot; 时，我应该多关注 &quot;How&quot;，少关注 &quot;are&quot;。</li>
</ul>
</li>
<li><strong>Android 类比</strong>：
<ul>
<li>这不再是死记硬背传 <code>Bundle</code> 了。</li>
<li>这更像是一个 <strong>HashMap</strong> 或者 <strong>SQL 查询</strong>。</li>
<li>当 Decoder 需要生成下一个词时，它拥有对 Input 数据的<strong>随机访问权限 (Random Access)</strong>。它可以在 O(1) 的时间复杂度内直接索引到原始句子中相关的部分，而不需要在内存里苦苦回想。</li>
<li><strong>并行化</strong>：RNN 必须等上一个词处理完才能处理下一个（像单线程）；Transformer 可以同时处理所有词（像 RxJava/Coroutines 的并发处理），效率极高。</li>
</ul>
</li>
</ul>
<h3 id="%E6%80%BB%E7%BB%93" tabindex="-1">总结</h3>
<ul>
<li><strong>Seq2Seq (RNN)</strong>：像是一个<strong>串行</strong>的 <code>InputStream</code>，容易发生“内存溢出”或“数据丢失”（记不住长句子）。</li>
<li><strong>Transformer</strong>：像是一个<strong>并发</strong>的、带有<strong>索引</strong>的数据库查询，指哪打哪，精准且高效。</li>
</ul>
<p>这就是为什么现在所有的 AI（包括你手机里的 Google Assistant 或 Siri 的新版本）都在底层换成了 Transformer 架构。</p>
<p>刚才我们讲到了架构的对比（Seq2Seq vs Transformer）。现在我们深入挖掘一下文本中提到的<strong>历史背景</strong>和<strong>技术痛点</strong>，看看为什么 Google 最终抛弃了旧架构，转向了 Transformer。</p>
<h3 id="1.-%E5%8E%86%E5%8F%B2%E8%83%8C%E6%99%AF%EF%BC%9Agoogle-translate-%E7%9A%84%E4%B8%80%E6%AC%A1%E2%80%9C%E9%87%8D%E6%9E%84%E2%80%9D" tabindex="-1">1. 历史背景：Google Translate 的一次“重构”</h3>
<p>文中提到：<em>“In 2016, Google incorporated seq2seq into Google Translate...”</em></p>
<ul>
<li><strong>背景故事</strong>：
在 2014-2016 年之前，机器翻译主要靠“统计机器翻译”（SMT），你可以理解为写了无数个 <code>if-else</code> 规则和查字典。
2016 年，Google 把翻译引擎的底层代码重构了，换成了 <strong>Seq2Seq (基于 RNN)</strong>。这在当时是一个巨大的飞跃，翻译质量大幅提升。</li>
<li><strong>Android 类比</strong>：
这就像 Android 5.0 (Lollipop) 时代，Google 把虚拟机从 <strong>Dalvik</strong> 换成了 <strong>ART</strong>。
虽然 ART (Seq2Seq) 比 Dalvik (旧规则) 强很多，但它依然有性能瓶颈，还不是最终形态。</li>
</ul>
<h3 id="2.-seq2seq-%E7%9A%84%E8%87%B4%E5%91%BD-bug%EF%BC%9A%E9%82%A3%E4%B8%AA%E8%93%9D%E8%89%B2%E7%9A%84%E6%96%B9%E5%9D%97" tabindex="-1">2. Seq2Seq 的致命 Bug：那个蓝色的方块</h3>
<p>文中提到：<em>“The decoder... conditioned on both the final hidden state...”</em> 以及图片上半部分中间那个蓝色的方块 <strong>&quot;Final hidden state&quot;</strong>。</p>
<p>这是旧架构最大的痛点，也是 Transformer 诞生的原因。</p>
<ul>
<li><strong>技术痛点</strong>：
Seq2Seq 架构要求 Encoder（编码器）把读到的整句话（无论多长），都压缩进这一个 <strong>&quot;Final hidden state&quot;</strong> 向量里。
Decoder（解码器）在生成翻译时，只能盯着这个被压缩过的向量看。</li>
<li><strong>Android 类比</strong>：
想象你正在做一个<strong>图片上传</strong>功能。
<ul>
<li><strong>Seq2Seq 的做法</strong>：不管用户选的是一张 10KB 的头像，还是一个 1GB 的 4K 视频，你都强制把它压缩成一个 <strong>Fixed Size 的 ByteArray</strong>（比如固定 512 bytes），然后传给服务器。</li>
<li><strong>后果</strong>：
<ol>
<li><strong>信息丢失 (OOM/Data Loss)</strong>：如果句子很长（视频很大），压缩后的数据就会严重失真。前面的单词（视频开头）早就被后面的单词覆盖或遗忘了。</li>
<li><strong>无法并行 (Main Thread Block)</strong>：因为是串行处理（RNN），你必须等第 99 个词处理完，才能处理第 100 个词。这就像在主线程跑耗时任务，无法利用多核 CPU 并行加速。</li>
</ol>
</li>
</ul>
</li>
</ul>
<h3 id="3.-transformer-%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%EF%BC%9Aattention-(%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6)" tabindex="-1">3. Transformer 的解决方案：Attention (注意力机制)</h3>
<p>文中提到：<em>“...transformer architecture... which is based on the attention mechanism.”</em></p>
<p>这就是图片下半部分那些<strong>虚线箭头</strong>的含义。</p>
<ul>
<li><strong>核心逻辑</strong>：
Transformer 抛弃了“压缩成一个状态”的做法。它保留了输入句子的<strong>完整上下文</strong>。当它需要翻译某个词时，它会通过“注意力机制”去<strong>查询</strong>原句中相关的词。</li>
<li><strong>Android 类比</strong>：
<ul>
<li><strong>不再压缩</strong>：你不再把 1GB 视频压缩成 512 bytes 了，而是把原始数据完整地放在内存/缓存里。</li>
<li><strong>按需加载 (Lazy Loading / Paging)</strong>：当服务器（Decoder）需要处理视频的第 10 分钟时，它不需要解压整个文件，而是通过一个**索引（Attention）**直接跳转到第 10 分钟的数据块读取。</li>
<li><strong>全异步并行 (Coroutines/RxJava)</strong>：因为不需要等待上一个状态，Transformer 可以一次性把整句话扔进 GPU 里并行计算。就像你用 <code>Coroutines</code> 并发发起了 10 个网络请求，而不是一个接一个地 <code>await</code>。</li>
</ul>
</li>
</ul>
<h3 id="4.-%E6%80%BB%E7%BB%93%EF%BC%9A%E6%96%87%E6%9C%AB%E7%9A%84%E4%BC%8F%E7%AC%94" tabindex="-1">4. 总结：文末的伏笔</h3>
<p>图片最后一句写道：<em>“There are two problems with seq2seq that Vaswani et al. (2017) addresses.”</em>
（Vaswani et al. 2017 就是那篇著名的论文《Attention Is All You Need》，AI 界的圣经）。</p>
<p>这两个问题（虽然图里没写完，但作为 Android 工程师你可以这样理解）：</p>
<ol>
<li><strong>长距离依赖问题 (Long-term Dependency)</strong>：
<ul>
<li><strong>Seq2Seq</strong>：句子太长就忘了开头（内存泄漏/被回收）。</li>
<li><strong>Transformer</strong>：无论句子多长，都能瞬间索引到开头（全局引用，O(1) 访问）。</li>
</ul>
</li>
<li><strong>训练效率问题 (Training Efficiency)</strong>：
<ul>
<li><strong>Seq2Seq</strong>：串行计算，GPU 只能用 1% 的算力，在那干等（单线程）。</li>
<li><strong>Transformer</strong>：并行计算，跑满 GPU 100% 算力（多线程并发）。</li>
</ul>
</li>
</ol>
<p>所以，现在的 ChatGPT、Claude、DeepSeek，本质上都是在这个“并发 + 索引”的高级架构上堆出来的超级应用。</p>
<h2 id="q%2Fk%2Fv" tabindex="-1">Q/K/V</h2>
<p>这页 PPT 讲的是 Transformer 架构中最核心、最天才的部分：<strong>Attention Mechanism（注意力机制）</strong>，具体来说是 <strong>Query (Q), Key (K), Value (V)</strong> 的概念。</p>
<p>对于 Android 工程师来说，理解这个机制最好的方式就是把它想象成一个 <strong>“超级模糊搜索的 HashMap”</strong>。</p>
<p>我们来拆解一下这三个核心变量：<strong>Q, K, V</strong>。</p>
<h3 id="1.-%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%EF%BC%9Aq%2C-k%2C-v-%E7%9A%84-android-%E6%98%A0%E5%B0%84" tabindex="-1">1. 核心概念：Q, K, V 的 Android 映射</h3>
<p>在传统的编程里，我们查数据通常是用 <code>HashMap</code>：
<code>Value result = map.get(Key);</code> —— 这是一个<strong>精确匹配</strong>，要么拿到，要么是 null。</p>
<p>但在 AI 的世界里，查找是<strong>模糊</strong>的。</p>
<ul>
<li>
<p><strong>Query (Q) —— “搜索请求”</strong></p>
<ul>
<li><strong>定义</strong>：当前模型正在关注的词（或者说正在生成的词）。</li>
<li><strong>Android 类比</strong>：用户在 <code>SearchView</code> 里输入的搜索关键词。</li>
<li><strong>图中例子</strong>：右边的红色框 $Q_t$。比如现在模型要翻译 &quot;How&quot; (对应西班牙语 &quot;Como&quot;)，那 $Q$ 就是 &quot;Como&quot; 的向量表示。它在问：“谁跟我有关？”</li>
</ul>
</li>
<li>
<p><strong>Key (K) —— “索引/标签”</strong></p>
<ul>
<li><strong>定义</strong>：之前所有输入过的词的“特征标签”。用来和 Q 进行匹配，看是否相关。</li>
<li><strong>Android 类比</strong>：你数据库（Room）里每一条数据的 <code>tag</code> 或 <code>id</code>。</li>
<li><strong>图中例子</strong>：橙色框 $K_1, K_2...$。代表 &quot;How&quot;, &quot;are&quot;, &quot;you&quot; 这些词的标签。</li>
</ul>
</li>
<li>
<p><strong>Value (V) —— “实际内容”</strong></p>
<ul>
<li><strong>定义</strong>：如果匹配上了，我要提取的实际信息内容。</li>
<li><strong>Android 类比</strong>：数据库里存的实际 JSON 数据对象（Payload）。</li>
<li><strong>图中例子</strong>：绿色框 $V_1, V_2...$。</li>
</ul>
</li>
</ul>
<h3 id="2.-%E6%B5%81%E7%A8%8B%E8%A7%A3%E6%9E%90%EF%BC%9A%E4%B8%80%E6%AC%A1%E2%80%9C%E5%8A%A0%E6%9D%83%E6%90%9C%E7%B4%A2%E2%80%9D%E7%9A%84%E8%BF%87%E7%A8%8B" tabindex="-1">2. 流程解析：一次“加权搜索”的过程</h3>
<p>看右边的流程图，这其实就是一次<strong>计算相关性并提取数据</strong>的过程：</p>
<ol>
<li>
<p><strong>Dot Product (点积) —— <code>calculateSimilarity()</code></strong></p>
<ul>
<li><strong>操作</strong>：拿 <strong>Query ($Q$)</strong> 去和每一个 <strong>Key ($K$)</strong> 做点积运算。</li>
<li><strong>Android 类比</strong>：遍历数据库，计算搜索词和每一条记录 tag 的<strong>相似度分数</strong>。</li>
<li><strong>结果</strong>：比如 &quot;Como&quot; (Q) 和 &quot;How&quot; (K) 的相似度很高（分数 0.9），和 &quot;?&quot; (K) 的相似度很低（分数 0.01）。</li>
</ul>
</li>
<li>
<p><strong>SoftMax —— <code>normalizeScores()</code></strong></p>
<ul>
<li><strong>操作</strong>：把上面的分数归一化，变成概率（加起来等于 1）。</li>
<li><strong>Android 类比</strong>：把原始分数转成百分比。</li>
<li><strong>结果</strong>：关注度分配 -&gt; 90% 给 &quot;How&quot;，5% 给 &quot;are&quot;，5% 给 &quot;you&quot;。</li>
</ul>
</li>
<li>
<p><strong>Weighted Sum (加权求和) —— <code>aggregateResults()</code></strong></p>
<ul>
<li><strong>操作</strong>：根据上面的百分比，把对应的 <strong>Value ($V$)</strong> 加起来。</li>
<li><strong>Android 类比</strong>：这不是返回一条数据，而是返回一个<strong>混合体</strong>。</li>
<li><strong>结果</strong>：最终输出 = 0.9 * (How的内容) + 0.05 * (are的内容) + ...</li>
<li>这就解释了为什么模型能理解上下文：它在生成下一个词时，**“混合”**了之前所有相关词的信息。</li>
</ul>
</li>
</ol>
<h3 id="3.-%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E6%9C%89%E6%80%A7%E8%83%BD%E7%93%B6%E9%A2%88%EF%BC%9F%EF%BC%88%E5%8F%B3%E4%B8%8B%E8%A7%92%E6%96%87%E5%AD%97%EF%BC%89" tabindex="-1">3. 为什么会有性能瓶颈？（右下角文字）</h3>
<p>文中提到：<em>“the longer the sequence, the more key and value vectors need to be computed and stored.”</em></p>
<p>这对 Android 工程师来说非常容易理解，这就是典型的 <strong>OOM (Out of Memory) 风险</strong> 和 <strong>缓存膨胀</strong>。</p>
<ul>
<li><strong>KV Cache (KV 缓存)</strong>：
为了不每次都重新计算，模型会把之前算好的 K 和 V 存起来。</li>
<li><strong>Context Window (上下文窗口)</strong>：
这就好比你的 <code>LruCache</code> 或者 <code>ArrayList</code>。
<ul>
<li>如果你聊了 100 句，就有 100 个 K-V 对。</li>
<li>如果你聊了 10 万句（比如丢给它一本书），这个 <code>ArrayList</code> 就会变得巨大无比，显存（RAM）直接撑爆。</li>
</ul>
</li>
<li><strong>计算复杂度 $O(N^2)$</strong>：
每生成一个新词，都要回头去和<strong>之前所有</strong>的词算一遍相似度（点积）。列表越长，遍历一次越慢。</li>
</ul>
<h3 id="%E6%80%BB%E7%BB%93-1" tabindex="-1">总结</h3>
<ul>
<li><strong>Attention</strong> 就是一个<strong>基于向量相似度的模糊查询系统</strong>。</li>
<li><strong>Q</strong> 是搜索词，<strong>K</strong> 是索引键，<strong>V</strong> 是数据值。</li>
<li>它不返回单一结果，而是返回所有相关结果的<strong>加权混合物</strong>。</li>
<li><strong>代价</strong>：对话越长，需要缓存的 K/V 越多，内存占用越大，计算越慢。这就是为什么现在的 AI 模型都有“最大上下文长度”（比如 128k token）的限制。</li>
</ul>
<h2 id="%E5%8F%82%E6%95%B0%E9%87%8F%E5%92%8C%E6%B7%B7%E5%90%88%E6%9E%B6%E6%9E%84%EF%BC%88moe%EF%BC%89" tabindex="-1">参数量和混合架构（MoE）</h2>
<p>这一页主要在讲 <strong>模型大小（Model Size）</strong> 以及一种特殊的架构 <strong>MoE（混合专家模型）</strong>。</p>
<p>作为 Android 工程师，你可以这样理解这一页的内容逻辑：</p>
<h3 id="%E5%B7%A6%E6%A0%8F%EF%BC%9A%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5-%E2%80%94%E2%80%94-%22apk-%E4%BD%93%E7%A7%AF%E4%B8%8E%E6%80%A7%E8%83%BD%E7%9A%84%E5%85%B3%E7%B3%BB%22" tabindex="-1">左栏：基础概念 —— &quot;APK 体积与性能的关系&quot;</h3>
<ol>
<li>
<p><strong>参数量 = 智力水平？</strong></p>
<ul>
<li><strong>原文片段</strong>：<em>“Llama-13B refers to... 13 billion parameters”</em></li>
<li><strong>核心观点</strong>：通常来说，参数越多，模型越强。比如 Llama-13B 通常比 Llama-7B 厉害。</li>
<li><strong>Android 类比</strong>：就像 APK 体积越大，通常包含的功能和资源越多。</li>
</ul>
</li>
<li>
<p><strong>新架构 &gt; 旧堆料（Note 部分）</strong></p>
<ul>
<li><strong>原文片段</strong>：<em>“Llama 3-8B (2024) outperforms even Llama 2-70B (2023)”</em></li>
<li><strong>核心观点</strong>：这很重要！它说现在的**小模型（8B）<strong>已经吊打去年的</strong>超大模型（70B）**了。</li>
<li><strong>Android 类比</strong>：这就像你重构了代码，用新的算法（Llama 3）写了一个 8MB 的 App，运行效率和功能比以前那个 70MB 的屎山代码（Llama 2）还要好。<strong>架构优化比单纯堆代码行数更重要。</strong></li>
</ul>
</li>
<li>
<p><strong>显存计算公式（底部）</strong></p>
<ul>
<li><strong>原文片段</strong>：<em>“each parameter is stored using 2 bytes... 14 billion bytes (14 GB)”</em></li>
<li><strong>核心观点</strong>：怎么算模型占多少内存？</li>
<li><strong>公式</strong>：参数量 × 2 (FP16精度) = 显存占用。</li>
<li><strong>例子</strong>：7B 模型 × 2 = 14GB 显存。</li>
<li><strong>Android 类比</strong>：这就是你在做端侧部署（On-device AI）时必须算的账。你的手机 RAM 够不够跑这个模型？</li>
</ul>
</li>
</ol>
<h3 id="%E5%8F%B3%E6%A0%8F%EF%BC%9A%E8%BF%9B%E9%98%B6%E6%9E%B6%E6%9E%84-%E2%80%94%E2%80%94-%22%E6%8F%92%E4%BB%B6%E5%8C%96%E4%B8%8E%E5%8A%A8%E6%80%81%E5%8A%A0%E8%BD%BD-(moe)%22" tabindex="-1">右栏：进阶架构 —— &quot;插件化与动态加载 (MoE)&quot;</h3>
<p>这部分被遮挡得最厉害，但讲的是目前最火的 <strong>MoE (Mixture of Experts)</strong> 架构，特别是 <strong>Mixtral 8x7B</strong> 这个模型。</p>
<ol>
<li>
<p><strong>什么是稀疏模型 (Sparse Model)？</strong></p>
<ul>
<li><strong>原文片段</strong>：<em>“zero-value parameters... 90% sparse”</em></li>
<li><strong>核心观点</strong>：有些模型虽然很大，但大部分参数是 0，或者不参与计算。</li>
</ul>
</li>
<li>
<p><strong>MoE (混合专家模型)</strong></p>
<ul>
<li><strong>原文片段</strong>：<em>“Mixture of Experts (MoE)... divided... groups of parameters... experts”</em></li>
<li><strong>核心观点</strong>：MoE 架构把一个大模型拆成了好几个“专家”（Experts）。</li>
<li><strong>Android 类比</strong>：
<ul>
<li><strong>传统模型 (Dense)</strong>：像是一个巨大的 <code>Monolithic App</code>，用户点一个按钮，整个 App 所有的代码都要过一遍，很慢，费电。</li>
<li><strong>MoE 模型</strong>：像是 <strong>“插件化架构”</strong> 或者 <strong>“动态 Feature Module”</strong>。App 里装了 8 个插件（8 Experts），但当用户点“翻译”按钮时，系统<strong>只动态加载并运行其中 2 个相关的插件</strong>，其他 6 个休眠。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Mixtral 8x7B 的魔法</strong></p>
<ul>
<li><strong>原文片段</strong>：<em>“Mixtral 8x7B... eight experts... only two experts are active... cost and speed are the same as a 12.9-billion... model”</em></li>
<li><strong>核心观点</strong>：
<ul>
<li><strong>总参数量</strong>：46.7B（虽然叫 8x7B，但因为有共享参数，不是 56B）。这是它的<strong>知识储备量</strong>（硬盘占用）。</li>
<li><strong>活跃参数量</strong>：12.9B（每次只用 2 个专家）。这是它的<strong>运行速度</strong>（CPU/内存占用）。</li>
</ul>
</li>
<li><strong>结论</strong>：MoE 模型让你拥有大模型的智商（47B），却只消耗小模型的算力（13B）。这就是为什么现在的端侧大模型都在往 MoE 方向发展。</li>
</ul>
</li>
<li>
<p><strong>数据的重要性（底部）</strong></p>
<ul>
<li><strong>原文片段</strong>：<em>“I like pineapples”</em></li>
<li><strong>核心观点</strong>：如果你用只有一句话的数据集去训练一个 13B 的大模型，它就是个傻子。<strong>数据质量决定上限</strong>，不仅仅是模型大小。</li>
</ul>
</li>
</ol>
<h3 id="%E6%80%BB%E7%BB%93-2" tabindex="-1">总结</h3>
<p>这一页书实际上是在教你如何评估一个模型：</p>
<ol>
<li><strong>看代数</strong>：Llama 3 比 Llama 2 强。</li>
<li><strong>算内存</strong>：参数量 × 2 bytes。</li>
<li><strong>看架构</strong>：MoE (Mixtral) 比传统模型更适合在资源受限的设备（如手机）上运行，因为它“平时不用的脑子就不转”，省电省算力。</li>
</ol>
<h2 id="llm-%E8%AE%AD%E7%BB%83%E8%A7%84%E6%A8%A1%E5%92%8C%E6%88%90%E6%9C%AC" tabindex="-1">LLM 训练规模和成本</h2>
<p>这两页在讲大模型训练中最烧钱、最硬核的两个维度：<strong>数据量 (Data)</strong> 和 <strong>算力 (Compute)</strong>。</p>
<p>作为 Android 工程师，你可以把这两页理解为：<strong>“为了编译这个超级 App，我们需要多大的代码库（数据），以及需要租多少台服务器跑多久（算力/成本）。”</strong></p>
<h3 id="%E7%AC%AC%E4%B8%80%E5%BC%A0%E5%9B%BE%EF%BC%88unqx%EF%BC%89%EF%BC%9A%E6%95%B0%E6%8D%AE%E9%87%8F-%E2%80%94%E2%80%94-%22%E4%BB%A3%E7%A0%81%E5%BA%93%E7%9A%84%E8%A7%84%E6%A8%A1%22" tabindex="-1">第一张图（UnqX）：数据量 —— &quot;代码库的规模&quot;</h3>
<p>这一页的核心是在讨论 <strong>Tokens（词元）</strong> 的数量。</p>
<ol>
<li>
<p><strong>衡量单位：Token vs Sample</strong></p>
<ul>
<li><strong>原文逻辑</strong>：以前大家用“样本数”（比如读了多少本书、多少个网页）来衡量数据量。但这不准，因为一本书可能很薄，也可能很厚。</li>
<li><strong>现在的标准</strong>：<strong>Token</strong>。</li>
<li><strong>Android 类比</strong>：
<ul>
<li><strong>Sample</strong>：就像你问“你的项目有多少个 <code>.java</code> 文件？”（文件大小不一，没法衡量工作量）。</li>
<li><strong>Token</strong>：就像你问“你的项目有多少行 <strong>代码 (LOC)</strong>？”或者“编译后的 <strong>Bytecode 指令数</strong>是多少？”。这是最精准的衡量单位。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>通货膨胀：Llama 的进化史</strong></p>
<ul>
<li><strong>Llama 1</strong>：1.4 Trillion (1.4万亿) tokens。</li>
<li><strong>Llama 2</strong>：2 Trillion tokens。</li>
<li><strong>Llama 3</strong>：15 Trillion tokens。</li>
<li><strong>解读</strong>：模型并没有变大太多（参数量），但它“读”的书变多了 10 倍。</li>
<li><strong>Android 类比</strong>：这就像你写同一个功能的 App，V1 版本你只参考了官方文档；V3 版本你把 GitHub 上所有相关的开源代码都读了一遍，写出来的代码肯定更健壮。</li>
</ul>
</li>
<li>
<p><strong>Chinchilla 定律（表格部分）</strong></p>
<ul>
<li>表格里提到了 <strong>Chinchilla</strong> 模型（70B 参数，1.4T tokens）。</li>
<li><strong>核心理论</strong>：这是 AI 界的一个黄金定律。它告诉我们，<strong>模型大小（脑容量）和训练数据量（阅读量）必须匹配</strong>。脑子太大书太少是浪费，脑子太小书太多读不完。</li>
<li><strong>Android 类比</strong>：<strong>CPU 性能与内存的配比</strong>。如果你给一个骁龙 8 Gen 3 的 CPU 配了 2GB 的 RAM，性能就瓶颈了；反之亦然。</li>
</ul>
</li>
</ol>
<h3 id="%E7%AC%AC%E4%BA%8C%E5%BC%A0%E5%9B%BE%EF%BC%88okv%EF%BC%89%EF%BC%9A%E7%AE%97%E5%8A%9B%E4%B8%8E%E6%88%90%E6%9C%AC-%E2%80%94%E2%80%94-%22%E7%BC%96%E8%AF%91%E8%80%97%E6%97%B6%E4%B8%8E%E4%BA%91%E6%9C%8D%E5%8A%A1%E8%B4%A6%E5%8D%95%22" tabindex="-1">第二张图（OKV）：算力与成本 —— &quot;编译耗时与云服务账单&quot;</h3>
<p>这一页非常硬核，在算<strong>钱</strong>和<strong>时间</strong>。</p>
<ol>
<li>
<p><strong>FLOPs vs FLOP/s（红色 Warning 框）</strong></p>
<ul>
<li>这是一个经典的坑。</li>
<li><strong>FLOPs (Floating Point Operations)</strong>：<strong>总量</strong>。指训练完这个模型一共需要做多少次浮点运算。
<ul>
<li><em>类比</em>：编译整个 AOSP 源码需要的 CPU 总指令数。</li>
</ul>
</li>
<li><strong>FLOP/s (per second)</strong>：<strong>速度</strong>。指你的显卡每秒能算多少次。
<ul>
<li><em>类比</em>：你的 CPU 主频（GHz）。</li>
</ul>
</li>
<li><strong>公式</strong>：<strong>训练时间 = 总工作量 (FLOPs) / 显卡速度 (FLOP/s)</strong>。</li>
</ul>
</li>
<li>
<p><strong>天价账单：训练 GPT-3 需要多久？</strong></p>
<ul>
<li><strong>假设条件</strong>：你有 <strong>256 张 H100 显卡</strong>（H100 是目前最强的 AI 显卡，单张售价约 3-4 万美元，这里光硬件就价值近千万美元）。</li>
<li><strong>计算结果</strong>：
<ul>
<li>如果你能跑满显卡性能，需要 <strong>236 天</strong>（约 8 个月）才能训练完一个 GPT-3 (175B)。</li>
</ul>
</li>
<li><strong>Android 类比</strong>：
<ul>
<li>想象一下，你点了一下 Android Studio 的 &quot;Run&quot; 按钮（开始训练）。</li>
<li>然后 Gradle Build 进度条开始走。</li>
<li>这个进度条要走 <strong>8 个月</strong> 才能编译完成。</li>
<li>而且中间不能断电，不能报错（OOM），否则可能要重头来过（Checkpoints 机制就是为了防止这个）。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>利用率 (Utilization) 的陷阱</strong></p>
<ul>
<li><strong>原文</strong>：<em>“Generally, if you can get half the advertised performance... you're doing okay.”</em></li>
<li><strong>解释</strong>：虽然显卡理论速度很快，但因为数据传输、网络延迟等原因，实际利用率通常只有 <strong>50% - 70%</strong>。</li>
<li><strong>Android 类比</strong>：就像你的 CPU 是 8 核的，但因为 I/O 阻塞或者锁竞争，实际运行时 CPU 利用率可能只有 40%，大部分时间都在等待。</li>
</ul>
</li>
<li>
<p><strong>最终成本（右下角）</strong></p>
<ul>
<li><strong>原文</strong>：<em>“cost over $4 million”</em></li>
<li><strong>结论</strong>：按每小时租金计算，训练一次 GPT-3 的电费和设备租金超过 <strong>400 万美元</strong>。</li>
<li><strong>Android 类比</strong>：这不再是个人开发者能玩的领域了。这就像是开发操作系统，只有 Google、Apple 这种巨头才付得起这个“编译费”。</li>
</ul>
</li>
</ol>
<h3 id="%E6%80%BB%E7%BB%93-3" tabindex="-1">总结</h3>
<p>这两页书揭示了为什么大模型是“富人的游戏”：</p>
<ol>
<li>你需要准备 <strong>万亿级别</strong> 的高质量代码/文本（Tokens）。</li>
<li>你需要租用 <strong>数百张</strong> 顶级显卡，连续跑 <strong>大半年</strong>。</li>
<li>只要代码有一行写错（参数设置不对），几百万美元的电费就打水漂了。</li>
</ol>
<h2 id="checkpoint-%E4%BB%8B%E7%BB%8D" tabindex="-1">Checkpoint 介绍</h2>
<p>结合刚才提到的“训练一次 GPT-3 需要 400 万美元、耗时 8 个月”的背景，<strong>Checkpoint（检查点）机制</strong>就是这场昂贵赌博中的**“存档”**功能。</p>
<p>如果没有 Checkpoint，一旦在第 7 个月零 29 天的时候机房停电或者显卡烧了，那 400 万美元就直接归零。</p>
<p>作为一个 Android 工程师，你可以通过以下几个维度来理解 Checkpoint：</p>
<h3 id="1.-%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%EF%BC%9A%E5%AE%83%E6%98%AF-ai-%E7%95%8C%E7%9A%84-onsaveinstancestate()" tabindex="-1">1. 核心概念：它是 AI 界的 <code>onSaveInstanceState()</code></h3>
<p>在 Android 开发中，当 Activity 因为内存不足被系统杀掉，或者用户旋转屏幕时，系统会调用 <code>onSaveInstanceState(Bundle)</code>。</p>
<ul>
<li><strong>目的</strong>：保存当前界面的状态（输入框里的文字、滚动条的位置）。</li>
<li><strong>结果</strong>：当 Activity 重建时，可以通过 <code>onRestoreInstanceState</code> 恢复现场，用户感觉不到中断。</li>
</ul>
<p><strong>Checkpoint 也是一样的逻辑：</strong>
训练大模型是一个漫长的循环（Loop）。每隔一段时间（比如每跑完 1000 个 Step，或者每 1 个小时），程序就会把内存里的所有数据 Dump 到硬盘上，生成一个文件（通常是 <code>.pt</code>, <code>.ckpt</code>, <code>.safetensors</code> 格式）。</p>
<h3 id="2.-checkpoint-%E5%88%B0%E5%BA%95%E5%AD%98%E4%BA%86%E4%BB%80%E4%B9%88%EF%BC%9F" tabindex="-1">2. Checkpoint 到底存了什么？</h3>
<p>它不仅仅是保存了模型的“智商”（权重），还保存了“学习的状态”。</p>
<ul>
<li><strong>Model Weights (模型权重)</strong>：
<ul>
<li><em>类比</em>：<code>EditText</code> 里用户已经输入的文字。</li>
<li>这是模型学到的知识，是推理（Inference）时唯一需要的东西。</li>
</ul>
</li>
<li><strong>Optimizer State (优化器状态)</strong>：
<ul>
<li><em>类比</em>：光标的位置、滚动条的惯性速度。</li>
<li>这是关键！训练是用“梯度下降”算法的，往往带有<strong>动量 (Momentum)</strong>。如果你只存权重，恢复训练时就像车子突然从 100km/h 刹停再重新起步，效率会大打折扣。保存优化器状态，是为了让车子能以 100km/h 的速度<strong>无缝接续</strong>跑下去。</li>
</ul>
</li>
<li><strong>Epoch / Step Number</strong>：
<ul>
<li><em>类比</em>：视频播放进度条的时间戳。告诉程序“我们上次学到第几章了”。</li>
</ul>
</li>
</ul>
<h3 id="3.-checkpoint-%E7%9A%84%E4%B8%89%E5%A4%A7%E6%A0%B8%E5%BF%83%E7%94%A8%E9%80%94" tabindex="-1">3. Checkpoint 的三大核心用途</h3>
<h4 id="a.-%E5%AE%B9%E7%81%BE%E4%B8%8E%E6%96%AD%E7%82%B9%E7%BB%AD%E8%AE%AD-(disaster-recovery)" tabindex="-1">A. 容灾与断点续训 (Disaster Recovery)</h4>
<p>这是最基础的功能。</p>
<ul>
<li><strong>场景</strong>：训练集群里有 1000 张显卡，概率学告诉我们，连续 8 个月不坏一张卡的概率几乎为 0。</li>
<li><strong>操作</strong>：当某张卡挂了导致训练崩溃，工程师换好卡，读取最新的 Checkpoint，可能只损失了最近 1 小时的进度，而不是 8 个月。</li>
<li><strong>Android 类比</strong>：玩 RPG 游戏打 Boss 前存个档。挂了就读档重来，不用从一级开始练号。</li>
</ul>
<h4 id="b.-%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88-(early-stopping-%2F-model-selection)" tabindex="-1">B. 防止过拟合 (Early Stopping / Model Selection)</h4>
<ul>
<li><strong>场景</strong>：模型训练并不是越久越好。有时候练到第 100 个 Epoch 效果最好，练到第 120 个 Epoch 反而因为“死记硬背”（过拟合）变笨了。</li>
<li><strong>操作</strong>：我们会保存很多个 Checkpoint（如 <code>epoch_10.ckpt</code>, <code>epoch_20.ckpt</code>...）。训练结束后，我们在测试集上跑分，发现 <code>epoch_100</code> 分数最高，那就选它发布，把后面的都删了。</li>
<li><strong>Android 类比</strong>：<strong>Git 的 Commit 历史</strong>。你写代码写嗨了，改出 Bug 了，你可以 <code>git checkout</code> 回退到昨天那个“虽然功能少点但能稳定运行”的版本。</li>
</ul>
<h4 id="c.-%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%BE%AE%E8%B0%83-(fine-tuning)" tabindex="-1">C. 迁移学习与微调 (Fine-tuning)</h4>
<p>这是目前开源社区（如 HuggingFace）最常用的方式。</p>
<ul>
<li><strong>场景</strong>：Meta 花了 400 万美元练好了 Llama-3-Base（基础模型），发布了一个 Checkpoint。</li>
<li><strong>操作</strong>：你下载这个 Checkpoint，加载到你的显卡里，然后喂给它你们公司的私有数据，继续训练（Fine-tune）。</li>
<li><strong>Android 类比</strong>：<strong>Fork 开源项目</strong>。你不需要从零写一个 OkHttp，你只需要 clone 下来（下载 Checkpoint），然后根据你的需求修改几行代码（微调），就能用了。</li>
</ul>
<h3 id="4.-%E6%80%BB%E7%BB%93" tabindex="-1">4. 总结</h3>
<p>对于 Android 工程师来说：</p>
<ul>
<li><strong>Training (训练)</strong> = 这是一个耗时 8 个月的 <code>AsyncTask</code> 或 <code>WorkManager</code> 任务。</li>
<li><strong>Checkpoint</strong> = 定期把任务进度序列化写入 <code>SharedPreferences</code> 或 <code>SQLite</code>。</li>
<li><strong>Inference (推理)</strong> = 任务结束，把最终生成的 <code>JSON</code> (模型权重) 打包进 APK 发布给用户。</li>
</ul>
<h2 id="%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E7%9A%84%E6%9C%80%E4%BD%B3%E5%8C%96%E5%92%8C%E7%93%B6%E9%A2%88" tabindex="-1">模型训练的最佳化和瓶颈</h2>
<p>这三页从单纯的技术架构转向了<strong>工程经济学</strong>和<strong>未来瓶颈</strong>。</p>
<p>作为 Android 工程师，你可以把这部分内容看作是：<strong>“如何用有限的预算（算力/钱）构建性能最好的 App，以及我们即将面临的‘资源枯竭’危机。”</strong></p>
<p>我为你拆解为三个核心模块：</p>
<h3 id="1.-%E9%BB%84%E9%87%91%E6%B3%95%E5%88%99%EF%BC%9Achinchilla-scaling-law%EF%BC%88%E7%AC%AC%E4%B8%80%E9%A1%B5%EF%BC%89" tabindex="-1">1. 黄金法则：Chinchilla Scaling Law（第一页）</h3>
<p>这一页解决了一个核心的工程问题：<strong>我有 1000 万美元预算（算力），我是该造一个巨大的模型（参数多），还是造一个中等模型但让它读更多的书（数据多）？</strong></p>
<ul>
<li>
<p><strong>Chinchilla 定律 (The Golden Rule)</strong>：</p>
<ul>
<li>DeepMind 的研究发现，<strong>模型大小</strong>和<strong>训练数据量</strong>必须按比例增长。</li>
<li><strong>黄金比例</strong>：<strong>训练 Token 数 $\approx$ 20 倍 × 模型参数量</strong>。</li>
<li><em>Android 类比</em>：这就像 <strong>CPU 和 RAM 的最佳配比</strong>。如果你给一个低端 CPU 配了 32GB RAM，或者给骁龙 8 Gen 3 配了 2GB RAM，都是浪费钱。只有 1:20 这个比例性价比最高。</li>
</ul>
</li>
<li>
<p><strong>Llama 的“违规”操作 (Inference Optimization)</strong>：</p>
<ul>
<li>文中提到 Llama 模型并没有遵守这个定律，它用了<strong>远超 20 倍</strong>的数据去训练一个小模型。为什么？</li>
<li><strong>为了“运行时”优化</strong>：虽然训练时多花了钱，但生成的模型更小（比如 8B），用户跑起来（Inference）更快、更省显存。</li>
<li><em>Android 类比</em>：这就像你为了减小 APK 体积和提升启动速度，在编译阶段开启了<strong>最高级别的混淆和优化 (R8 full mode)</strong>。虽然编译时间（训练成本）增加了 3 倍，但用户安装后的运行体验（推理成本）变好了。这是典型的<strong>以“编译时间”换“运行效率”</strong>。</li>
</ul>
</li>
</ul>
<h3 id="2.-%E8%B0%83%E5%8F%82%E7%8E%84%E5%AD%A6%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%9E%AF%E7%AB%AD%EF%BC%88%E7%AC%AC%E4%BA%8C%E9%A1%B5%EF%BC%89" tabindex="-1">2. 调参玄学与数据枯竭（第二页）</h3>
<p>这一页讲了两个让工程师头大的问题：<strong>参数怎么调</strong>以及<strong>数据不够用了</strong>。</p>
<ul>
<li>
<p><strong>Parameter vs. Hyperparameter (参数 vs 超参数)</strong>：</p>
<ul>
<li><strong>Parameter</strong>：模型自己学出来的权重（比如神经网络里的数字）。
<ul>
<li><em>类比</em>：数据库里的用户数据。</li>
</ul>
</li>
<li><strong>Hyperparameter</strong>：工程师在训练前手动设置的配置（学习率、Batch Size、层数）。
<ul>
<li><em>类比</em>：你的 <code>build.gradle</code> 配置（<code>minSdk</code>, <code>versionCode</code>, <code>jvmTarget</code>）。一旦配置错了（比如学习率太高），整个项目编译（训练）就会失败。</li>
</ul>
</li>
<li><strong>难点</strong>：超参数组合有无数种，你不可能每次都重新编译一遍来测试。所以现在流行“小模型试错，大模型照搬”（Scaling Extrapolation）。</li>
</ul>
</li>
<li>
<p><strong>数据枯竭 (The Data Wall)</strong>：</p>
<ul>
<li>看那个图表（Figure 2-9）。蓝色的线是<strong>现存的高质量人类文本数据</strong>，虚线是<strong>模型对数据的需求量</strong>。</li>
<li><strong>结论</strong>：两条线即将在 <strong>2026-2028 年</strong> 交叉。</li>
<li><em>Android 类比</em>：<strong>“流量红利见顶”</strong>。以前随便做一个 App 都有用户增长，现在存量市场竞争，没有新用户（新数据）了。AI 快要把互联网上所有人类写过的字都读完了。</li>
</ul>
</li>
</ul>
<h3 id="3.-%E4%B9%B1%E4%BC%A6%E5%8D%B1%E6%9C%BA%E4%B8%8E%E8%83%BD%E6%BA%90%E5%8D%B1%E6%9C%BA%EF%BC%88%E7%AC%AC%E4%B8%89%E9%A1%B5%EF%BC%89" tabindex="-1">3. 乱伦危机与能源危机（第三页）</h3>
<p>这一页讲的是未来的两大隐患，非常赛博朋克。</p>
<ul>
<li>
<p><strong>AI 吃 AI (Model Collapse / Ouroboros)</strong>：</p>
<ul>
<li><strong>现象</strong>：现在的互联网充斥着 AI 生成的内容。如果新一代 AI 用这些数据训练，就会出现“近亲繁殖”。</li>
<li>文中提到 Grok 拒绝回答问题，因为它是在 ChatGPT 生成的数据上训练的，学到了 OpenAI 的拒绝策略。</li>
<li><em>Android 类比</em>：<strong>JPEG 压缩伪影</strong>。你把一张图压缩成 JPEG，截图，再压缩，再截图……重复 10 次，图片就糊得没法看了。模型如果只吃 AI 生成的“垃圾食品”，智商会退化。</li>
</ul>
</li>
<li>
<p><strong>数据围墙 (Data Wars)</strong>：</p>
<ul>
<li>Reddit, StackOverflow 开始收费或禁止爬虫。</li>
<li><em>Android 类比</em>：<strong>API 免费时代结束</strong>。以前你可以随便调用的 Open API，现在全部变成了 Enterprise Paid Plan，而且有严格的 Rate Limit。</li>
</ul>
</li>
<li>
<p><strong>能源危机 (Electricity)</strong>：</p>
<ul>
<li>数据中心现在消耗全球 1-2% 的电力，预计 2030 年会达到 <strong>4-20%</strong>。</li>
<li><em>Android 类比</em>：你的 App 优化得再好，如果它运行一小时要消耗手机 50% 的电量，那它就是不可用的。现在的 AI 正在面临这个问题——<strong>电不够用了</strong>。</li>
</ul>
</li>
</ul>
<h3 id="%E6%80%BB%E7%BB%93-4" tabindex="-1">总结</h3>
<p>这三页书告诉你，做 AI 不仅仅是写代码，现在面临的是：</p>
<ol>
<li><strong>算账</strong>：怎么用最少的钱训练出最强的模型（Chinchilla 定律）。</li>
<li><strong>抢资源</strong>：抢高质量的人类数据（防止 AI 吃 AI），抢电力。</li>
<li><strong>拼落地</strong>：像 Llama 那样，宁可训练时多花钱，也要把模型做小，方便在端侧（手机/PC）运行。</li>
</ol>
<p>追问：</p>
<h3 id="%E9%97%AE%E9%A2%98%E4%B8%80%EF%BC%9A%E6%80%8E%E4%B9%88%E9%A2%84%E8%AE%BE%2F%E6%8E%A8%E6%96%AD%E6%88%91%E8%A6%81%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AA%E5%A4%9A%E5%A4%A7%E7%9A%84%E6%A8%A1%E5%9E%8B%EF%BC%9F" tabindex="-1">问题一：怎么预设/推断我要训练一个多大的模型？</h3>
<p>决定模型大小（参数量，比如 7B, 70B, 175B）并不是拍脑袋决定的，它通常是一个**“倒推法”**的过程。</p>
<p>你可以把它想象成：<strong>“我要开发一个 App，我该把 <code>minSdk</code> 设为多少？APK 体积限制是多少？”</strong></p>
<p>主要有三个维度的限制条件，通常是<strong>短板效应</strong>决定了你的上限：</p>
<h4 id="1.-%E7%A1%AC%E4%BB%B6%E9%99%90%E5%88%B6%EF%BC%88%E6%9C%80%E7%A1%AC%E7%9A%84%E6%8C%87%E6%A0%87%EF%BC%89%E2%80%94%E2%80%94-%22%E7%94%A8%E6%88%B7%E7%9A%84%E6%89%8B%E6%9C%BA%E5%B8%A6%E5%BE%97%E5%8A%A8%E5%90%97%EF%BC%9F%22" tabindex="-1">1. 硬件限制（最硬的指标）—— &quot;用户的手机带得动吗？&quot;</h4>
<p>这是最直接的判断标准，特别是对于端侧（On-device）模型。</p>
<ul>
<li><strong>显存/内存公式</strong>：模型参数量 × 2 Bytes (FP16精度) = 最小显存需求。
<ul>
<li><strong>7B 模型</strong> $\approx$ 14GB 显存。</li>
<li><strong>13B 模型</strong> $\approx$ 26GB 显存。</li>
</ul>
</li>
<li><strong>Android 场景</strong>：
<ul>
<li>如果你想让模型跑在 Pixel 8 或高端小米手机上（通常 12GB-16GB RAM，但系统和 App 要吃掉一半），你撑死只能跑一个 <strong>3B - 7B</strong> 的量化版模型（4-bit 量化后体积减半）。</li>
<li><strong>结论</strong>：如果目标是手机端，<strong>7B 是目前的物理极限</strong>，不用想更大的了。</li>
</ul>
</li>
</ul>
<h4 id="2.-%E6%95%B0%E6%8D%AE%E9%87%8F%E9%99%90%E5%88%B6%EF%BC%88chinchilla-%E5%AE%9A%E5%BE%8B%EF%BC%89%E2%80%94%E2%80%94-%22%E6%88%91%E6%9C%89%E5%A4%9A%E5%B0%91%E7%A0%96%E5%A4%B4%E7%9B%96%E6%88%BF%E5%AD%90%EF%BC%9F%22" tabindex="-1">2. 数据量限制（Chinchilla 定律）—— &quot;我有多少砖头盖房子？&quot;</h4>
<p>如果你有无限的算力（比如你是 Google），那么限制你的就是数据量。</p>
<ul>
<li><strong>黄金法则</strong>：<strong>训练数据 Token 数应该是参数量的 20 倍。</strong></li>
<li><strong>推断逻辑</strong>：
<ul>
<li>假设你手里只有 <strong>100 亿 (10B)</strong> 个 Token 的高质量行业数据（比如你们公司的所有文档）。</li>
<li>根据公式：$10B \div 20 = 0.5B$。</li>
<li><strong>结论</strong>：你应该训练一个 <strong>0.5B (5亿参数)</strong> 的小模型。如果你强行训练一个 7B 的模型，它就像一个脑容量很大但没书读的学生，会产生严重的“幻觉”或过拟合。</li>
</ul>
</li>
</ul>
<h4 id="3.-%E7%AE%97%E5%8A%9B%E9%A2%84%E7%AE%97%EF%BC%88%E8%80%81%E6%9D%BF%E7%BB%99%E5%A4%9A%E5%B0%91%E9%92%B1%EF%BC%89%E2%80%94%E2%80%94-%22%E9%A1%B9%E7%9B%AE%E7%BB%8F%E8%B4%B9%E5%A4%9F%E7%83%A7%E5%87%A0%E5%A4%A9%EF%BC%9F%22" tabindex="-1">3. 算力预算（老板给多少钱）—— &quot;项目经费够烧几天？&quot;</h4>
<p>这是最现实的商业逻辑。</p>
<ul>
<li><strong>估算公式</strong>：训练成本 $\propto$ 模型大小 × 数据量。</li>
<li><strong>推断逻辑</strong>：
<ul>
<li>老板说：“给你 1 万美元预算。”</li>
<li>你去租 H100 显卡，算一下 1 万美元能跑多少个 FLOPs。</li>
<li>然后查表（Chinchilla Scaling Law 图表），看这点算力能支持的最佳模型是多少。</li>
<li><strong>结论</strong>：可能算下来只能训练一个 <strong>1B</strong> 的模型。</li>
</ul>
</li>
</ul>
<p><strong>总结：如何决定？</strong></p>
<ol>
<li><strong>先看部署环境</strong>：跑在服务器？（上限很高）还是跑在手机？（上限 7B）。</li>
<li><strong>再看数据口袋</strong>：数据够不够填满这个模型？（不够就缩小模型）。</li>
<li><strong>最后看钱包</strong>：租得起显卡吗？</li>
</ol>
<hr>
<h3 id="%E9%97%AE%E9%A2%98%E4%BA%8C%EF%BC%9Aparameter-(%E5%8F%82%E6%95%B0)-vs.-hyperparameter-(%E8%B6%85%E5%8F%82%E6%95%B0)" tabindex="-1">问题二：Parameter (参数) vs. Hyperparameter (超参数)</h3>
<p>这个概念确实容易混淆。我们用 <strong>Android 项目编译</strong> 和 <strong>教学生</strong> 两个类比来彻底区分。</p>
<h4 id="%E7%B1%BB%E6%AF%94-1%EF%BC%9Aandroid-%E9%A1%B9%E7%9B%AE%E6%9E%84%E5%BB%BA-(build-process)" tabindex="-1">类比 1：Android 项目构建 (Build Process)</h4>
<p>想象你在开发一个 App。</p>
<ul>
<li>
<p><strong>Hyperparameter (超参数) = <code>build.gradle</code> 配置</strong></p>
<ul>
<li><strong>谁设定的？</strong> <strong>你（开发者）</strong>。</li>
<li><strong>什么时候设定的？</strong> 在点击 &quot;Run&quot; <strong>之前</strong>。</li>
<li><strong>特点</strong>：一旦开始编译（训练），这些值就<strong>不能变</strong>了。</li>
<li><strong>例子</strong>：
<ul>
<li><code>compileSdk</code> (对应 AI 的网络层数：决定了架构的复杂度)</li>
<li><code>minSdk</code> (对应 AI 的 Batch Size：一次处理多少数据)</li>
<li><code>versionCode</code> (对应 AI 的 Epochs：训练多少轮)</li>
<li><code>learning_rate</code> (对应 AI 的学习率：步子迈多大)</li>
</ul>
</li>
<li><strong>作用</strong>：它们指导编译器（训练程序）<strong>如何</strong>去工作。</li>
</ul>
</li>
<li>
<p><strong>Parameter (参数) = <code>classes.dex</code> (编译后的字节码)</strong></p>
<ul>
<li><strong>谁设定的？</strong> <strong>编译器（训练算法/梯度下降）</strong>。你<strong>不能</strong>手动去写字节码。</li>
<li><strong>什么时候设定的？</strong> 在编译（训练）<strong>过程中</strong>自动生成的。</li>
<li><strong>特点</strong>：它们是成千上万个看不懂的数字（权重矩阵）。</li>
<li><strong>例子</strong>：神经网络里每一个神经元的连接权重（Weight）和偏置（Bias）。</li>
<li><strong>作用</strong>：它们是最终的<strong>产物</strong>。App 运行逻辑全靠它们。</li>
</ul>
</li>
</ul>
<h4 id="%E7%B1%BB%E6%AF%94-2%EF%BC%9A%E6%95%99%E5%AD%A6%E7%94%9F%E8%80%83%E8%AF%95" tabindex="-1">类比 2：教学生考试</h4>
<p>想象你是一个老师（开发者），你要教一个学生（模型）去参加高考。</p>
<ul>
<li>
<p><strong>Hyperparameter (超参数) = 教学大纲与方法</strong></p>
<ul>
<li><strong>你决定的</strong>：
<ul>
<li>“每天上课 8 小时还是 12 小时？”（Batch Size）</li>
<li>“这本教材读 1 遍还是读 3 遍？”（Epochs）</li>
<li>“做错题时，是严厉批评还是温柔引导？”（Learning Rate / Optimizer）</li>
</ul>
</li>
<li>这些是你作为“上帝”视角设定的规则。</li>
</ul>
</li>
<li>
<p><strong>Parameter (参数) = 学生脑子里的知识点</strong></p>
<ul>
<li><strong>学生自己学的</strong>：
<ul>
<li>学生通过你设定的教学方法，自己领悟到了 $E=mc^2$。</li>
<li>学生脑子里的神经元连接发生了变化。</li>
</ul>
</li>
<li>你无法直接把知识“插”进学生脑子里，你只能通过调整教学方法（超参数），让学生自己去调整脑子里的知识（参数）。</li>
</ul>
</li>
</ul>
<h4 id="%E6%A0%B8%E5%BF%83%E5%8C%BA%E5%88%AB%E5%AF%B9%E7%85%A7%E8%A1%A8" tabindex="-1">核心区别对照表</h4>
<table>
<thead>
<tr>
<th style="text-align:left">特性</th>
<th style="text-align:left">Hyperparameter (超参数)</th>
<th style="text-align:left">Parameter (参数)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>设定者</strong></td>
<td style="text-align:left"><strong>人 (工程师)</strong></td>
<td style="text-align:left"><strong>机器 (算法)</strong></td>
</tr>
<tr>
<td style="text-align:left"><strong>设定时间</strong></td>
<td style="text-align:left">训练<strong>开始前</strong></td>
<td style="text-align:left">训练<strong>过程中</strong>不断更新</td>
</tr>
<tr>
<td style="text-align:left"><strong>数量级</strong></td>
<td style="text-align:left">几十个 (可枚举)</td>
<td style="text-align:left">几十亿、几千亿 (7B, 175B)</td>
</tr>
<tr>
<td style="text-align:left"><strong>Android 类比</strong></td>
<td style="text-align:left"><code>build.gradle</code> 配置</td>
<td style="text-align:left"><code>classes.dex</code> 里的字节码</td>
</tr>
<tr>
<td style="text-align:left"><strong>如何优化</strong></td>
<td style="text-align:left">调参 (Tuning) - 试错法</td>
<td style="text-align:left">训练 (Training) - 梯度下降</td>
</tr>
<tr>
<td style="text-align:left"><strong>例子</strong></td>
<td style="text-align:left">学习率、层数、Batch Size</td>
<td style="text-align:left">权重 (Weights)、偏置 (Biases)</td>
</tr>
</tbody>
</table>
<p><strong>一句话总结：</strong>
<strong>超参数 (Hyperparameter)</strong> 是你用来控制训练过程的**“旋钮”<strong>；
<strong>参数 (Parameter)</strong> 是训练结束后模型学到的</strong>“脑细胞”**。</p>
<h2 id="shoggoth%EF%BC%88%E4%BF%AE%E6%A0%BC%E6%96%AF%EF%BC%89%E6%A2%97%E5%9B%BE" tabindex="-1">Shoggoth（修格斯）梗图</h2>
<p>这四页内容非常经典，它们解释了<strong>为什么 ChatGPT 能像人一样聊天，而不是像搜索引擎一样只会补全句子</strong>。</p>
<p>这里引入了一个 AI 圈最著名的梗图——<strong>Shoggoth（修格斯）</strong>（那个长满眼睛的克苏鲁怪物）。</p>
<p>作为 Android 工程师，你可以把这部分理解为：<strong>“从 Linux 内核（Pre-training）到 Android UI（Post-training）的进化过程”</strong>。</p>
<h3 id="1.-%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%EF%BC%9Ashoggoth-%E9%9D%A2%E5%85%B7%EF%BC%88%E5%9B%BE%E4%BA%8C%E7%9A%84%E6%80%AA%E7%89%A9%EF%BC%89" tabindex="-1">1. 核心概念：Shoggoth 面具（图二的怪物）</h3>
<p>这是理解这几页的钥匙。</p>
<ul>
<li>
<p><strong>怪物本体 (Pre-trained Model)</strong>：</p>
<ul>
<li><strong>状态</strong>：它读完了互联网上所有的书，智商极高，但性格混乱邪恶。它不知道什么是“对话”，它只知道“补全”。</li>
<li><strong>例子</strong>：你问它“怎么制作炸弹？”，它可能会真的给你一个化学配方，或者补全成“怎么制作炸弹的教程在第几页”。</li>
<li><strong>Android 类比</strong>：<strong>AOSP 的底层源码</strong>。功能极其强大，能控制硬件，但如果你直接给普通用户一个 Terminal 终端，用户会疯掉。</li>
</ul>
</li>
<li>
<p><strong>面具 (SFT - Supervised Finetuning)</strong>：</p>
<ul>
<li><strong>状态</strong>：给怪物戴上了一个“人类”的面具。它学会了“一问一答”的格式。</li>
<li><strong>Android 类比</strong>：<strong>System UI / Launcher</strong>。用户终于看到了图标和按钮，知道怎么交互了。</li>
</ul>
</li>
<li>
<p><strong>笑脸 (RLHF - Preference Finetuning)</strong>：</p>
<ul>
<li><strong>状态</strong>：在面具上画了个笑脸。它不仅会回答，还变得礼貌、安全、政治正确。</li>
<li><strong>Android 类比</strong>：<strong>Google Play 审核机制 / SafetyNet</strong>。确保 App 不会崩溃，不会有恶意代码，符合社区规范。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="2.-post-training-%E7%9A%84%E4%B8%A4%E4%B8%AA%E9%98%B6%E6%AE%B5%EF%BC%88%E5%9B%BE%E4%B8%80-%26-%E5%9B%BE%E4%B8%89%EF%BC%89" tabindex="-1">2. Post-Training 的两个阶段（图一 &amp; 图三）</h3>
<p>文中明确指出，预训练（Pre-training）消耗了 98% 的算力，但**后训练（Post-training）**才是决定产品体验的关键 2%。</p>
<h4 id="%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5%EF%BC%9Asft-(%E6%9C%89%E7%9B%91%E7%9D%A3%E5%BE%AE%E8%B0%83)-%E2%80%94%E2%80%94-%22%E6%95%99%E5%AE%83%E8%AF%B4%E8%AF%9D%22" tabindex="-1">第一阶段：SFT (有监督微调) —— &quot;教它说话&quot;</h4>
<ul>
<li><strong>问题</strong>：预训练模型是“补全机”。
<ul>
<li>你输入：“如何做披萨”</li>
<li>它补全：“...给一家六口人吃？”（它以为你在写句子）</li>
</ul>
</li>
<li><strong>目标</strong>：让模型学会<strong>遵循指令 (Instruction Following)</strong>。
<ul>
<li>你输入：“如何做披萨”</li>
<li>它回答：“1. 准备面粉...”</li>
</ul>
</li>
<li><strong>方法</strong>：<strong>Behavior Cloning (行为克隆)</strong>。
<ul>
<li>找几万个真人，写出完美的“问题-答案”对（Prompt-Response Pairs），喂给模型吃。</li>
<li><strong>Android 类比</strong>：这就像你在写 <strong>Unit Test (单元测试)</strong>。你明确告诉函数：输入 A，必须输出 B。</li>
</ul>
</li>
</ul>
<h4 id="%E7%AC%AC%E4%BA%8C%E9%98%B6%E6%AE%B5%EF%BC%9Arlhf-(%E4%BA%BA%E7%B1%BB%E5%8F%8D%E9%A6%88%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0)-%E2%80%94%E2%80%94-%22%E6%95%99%E5%AE%83%E5%81%9A%E4%BA%BA%22" tabindex="-1">第二阶段：RLHF (人类反馈强化学习) —— &quot;教它做人&quot;</h4>
<ul>
<li><strong>问题</strong>：SFT 模型可能会一本正经地胡说八道，或者输出有害信息。</li>
<li><strong>目标</strong>：让模型的回答符合人类的<strong>偏好 (Preference)</strong>——有用、诚实、无害。</li>
<li><strong>方法</strong>：
<ol>
<li>模型生成两个答案。</li>
<li>人类标记员选出“更好的那个”。</li>
<li>训练一个“奖励模型 (Reward Model)”来模仿人类的打分标准。</li>
<li>用强化学习（RL）让模型不断刷高分。</li>
</ol>
</li>
<li><strong>Android 类比</strong>：这就像 <strong>A/B Testing</strong> 和 <strong>用户反馈系统</strong>。你发布了两个版本的 UI，看用户更喜欢点哪个，然后根据数据优化产品。</li>
</ul>
<hr>
<h3 id="3.-%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F%E7%9A%84%E4%BB%A3%E4%BB%B7%EF%BC%88%E5%9B%BE%E5%9B%9B%EF%BC%89" tabindex="-1">3. 数据质量的代价（图四）</h3>
<p>这一页揭示了为什么现在的 AI 越来越贵。</p>
<ul>
<li><strong>昂贵的“老师”</strong>：
<ul>
<li>做 SFT 数据标注的不是随便找的兼职。文中提到，OpenAI 的标注员 <strong>90% 拥有大学学位</strong>，甚至有很多硕士/博士。</li>
<li>写一条高质量的问答对（Prompt-Response），可能需要 <strong>30 分钟</strong>。</li>
</ul>
</li>
<li><strong>成本计算</strong>：
<ul>
<li>一条数据成本约 <strong>$10</strong>。</li>
<li>InstructGPT 用了 13,000 条数据，光这一小步的数据成本就是 <strong>$130,000</strong>（还不算管理费）。</li>
</ul>
</li>
<li><strong>Android 类比</strong>：
<ul>
<li><strong>Pre-training 数据</strong> = <strong>Logcat 里的原始日志</strong>（量大，但全是噪音）。</li>
<li><strong>SFT 数据</strong> = <strong>高级工程师写的 Technical Design Doc</strong>（量少，但字字珠玑，非常昂贵）。</li>
</ul>
</li>
</ul>
<h3 id="%E6%80%BB%E7%BB%93-5" tabindex="-1">总结</h3>
<p>这几页书解释了为什么你觉得 ChatGPT &quot;像人&quot;：</p>
<ol>
<li><strong>Pre-training</strong> 给了它<strong>智商</strong>（怪物的脑子）。</li>
<li><strong>SFT</strong> 给了它<strong>对话的能力</strong>（面具）。</li>
<li><strong>RLHF</strong> 给了它<strong>情商和道德</strong>（笑脸）。</li>
</ol>
<p>对于 Android 工程师来说，这就是从 <strong>Kernel (内核)</strong> 到 <strong>Framework (框架)</strong> 再到 <strong>Application (应用体验)</strong> 的完整封装过程。</p>
<p>SFT 和 RLHF 是不是老的 ML 时代就有，不是现在 AI 时代的产物？答案是：<strong>是的，它们绝对是“旧时代”的产物，或者更准确地说，是深度学习（Deep Learning）中期的经典技术，被“旧瓶装新酒”用在了 LLM 上。</strong></p>
<p>作为 Android 工程师，你可以这样理解：<strong>这就像是把十几年前就有的 <code>MVC</code> 模式和 <code>A/B Testing</code> 思想，突然用在了最新的 <code>Jetpack Compose</code> 架构里，产生奇效。</strong></p>
<p>我们来逐个“考古”：</p>
<h3 id="1.-sft-(supervised-fine-tuning)-%E2%80%94%E2%80%94-%E5%85%B6%E5%AE%9E%E5%B0%B1%E6%98%AF%E2%80%9C%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E2%80%9D" tabindex="-1">1. SFT (Supervised Fine-Tuning) —— 其实就是“迁移学习”</h3>
<p>SFT 并不是什么新词，它在“老 ML 时代”（2012-2018年，计算机视觉统治时期）有一个更通用的名字：<strong>Transfer Learning（迁移学习）</strong>。</p>
<ul>
<li><strong>考古现场</strong>：
<ul>
<li>早在 2014 年左右，做图像识别的标准流程就是：下载一个在 ImageNet（千万级通用图片库）上训练好的 <strong>ResNet</strong> 或 <strong>VGG</strong> 模型（相当于现在的 Pre-trained Model）。</li>
<li>然后，把最后一层分类层砍掉，换成你自己的分类层（比如识别“猫 vs 狗”）。</li>
<li>最后，用你手里少量的猫狗图片，去微调（Fine-tune）这个模型的参数。</li>
</ul>
</li>
<li><strong>本质</strong>：
这就是 SFT。<strong>Pre-training 学通用特征（边缘、纹理），Fine-tuning 学特定任务。</strong></li>
<li><strong>Android 类比</strong>：
这就像你继承了一个功能强大的 <code>BaseActivity</code>（预训练模型），然后重写了它的 <code>onCreate()</code> 方法（SFT），加入你自己页面的特定逻辑。你不需要从零开始写一个 Activity 类，你只是在“微调”它。</li>
</ul>
<h3 id="2.-rlhf-(reinforcement-learning-from-human-feedback)-%E2%80%94%E2%80%94-%E4%B9%9F%E5%B0%B1%E6%98%AF%E2%80%9C%E6%95%99%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%BF%BB%E8%B7%9F%E5%A4%B4%E2%80%9D" tabindex="-1">2. RLHF (Reinforcement Learning from Human Feedback) —— 也就是“教机器人翻跟头”</h3>
<p>RLHF 听起来很赛博朋克，但它的核心组件 <strong>RL（强化学习）</strong> 是 ML 里最古老的流派之一（AlphaGo 下围棋用的就是 RL）。</p>
<p>而 <strong>RLHF（带人类反馈的 RL）</strong> 这个具体组合，成名于 <strong>2017年</strong>（OpenAI 和 DeepMind 的论文）。</p>
<ul>
<li><strong>考古现场</strong>：
<ul>
<li>在 ChatGPT 出现之前，RLHF 主要不是用来教 AI 说话的，而是用来<strong>教机器人（Agent）做动作</strong>的。</li>
<li><strong>经典案例</strong>：2017 年，OpenAI 发表了一篇论文，他们想教一个火柴人做“后空翻”。但是写代码定义“什么是完美的后空翻”太难了（数学公式很难写）。</li>
<li><strong>解决方案</strong>：让 AI 随机动，生成两段视频。让人类看，选“哪一段更像后空翻”。AI 根据人类的选择（Reward）去调整动作。最后火柴人学会了极其标准的后空翻。</li>
</ul>
</li>
<li><strong>本质</strong>：
<strong>用“人类的选择”代替“数学公式”作为奖励函数（Reward Function）。</strong></li>
<li><strong>Android 类比</strong>：
这就像 <strong>Google Play 的推荐算法</strong>。
<ul>
<li>Google 很难写死一个公式说“用户喜欢什么样的 App”。</li>
<li>但是 Google 可以根据用户的 <strong>点击（Click）</strong> 和 <strong>卸载（Uninstall）</strong> 行为（Human Feedback），来强化推荐算法（RL）。用户点得越多，算法就越倾向于推这类 App。</li>
</ul>
</li>
</ul>
<h3 id="3.-%E4%B8%BA%E4%BB%80%E4%B9%88%E5%AE%83%E4%BB%AC%E7%8E%B0%E5%9C%A8%E7%AA%81%E7%84%B6%E2%80%9C%E7%81%AB%E2%80%9D%E4%BA%86%EF%BC%9F" tabindex="-1">3. 为什么它们现在突然“火”了？</h3>
<p>既然都是老技术，为什么以前没造出 ChatGPT？</p>
<p>因为以前大家<strong>用错了地方</strong>，或者说<strong>没打通任督二脉</strong>。</p>
<ol>
<li>
<p><strong>SFT 的转变</strong>：</p>
<ul>
<li><strong>以前</strong>：SFT 用来做<strong>分类</strong>（这张图是猫还是狗？）。</li>
<li><strong>现在</strong>：SFT 用来做<strong>生成</strong>（请写一首关于猫的诗）。</li>
<li><em>突破点</em>：人们发现，只要给大模型看几个“指令-回复”的例子，它就能学会“听懂指令”这种抽象能力。</li>
</ul>
</li>
<li>
<p><strong>RLHF 的跨界</strong>：</p>
<ul>
<li><strong>以前</strong>：RLHF 用来玩游戏、控制机器人手臂。</li>
<li><strong>现在</strong>：OpenAI 的天才之处在于，他们把这个用来“控制机器人动作”的技术，拿来<strong>控制“文本生成的概率”</strong>。</li>
<li><em>突破点</em>：他们发现，用 RLHF 调整模型，比单纯喂数据更能让模型学会“人类的价值观”（比如礼貌、不骂人）。</li>
</ul>
</li>
</ol>
<h3 id="%E6%80%BB%E7%BB%93-6" tabindex="-1">总结</h3>
<ul>
<li><strong>SFT</strong> = <strong>老派的迁移学习 (Transfer Learning)</strong>。</li>
<li><strong>RLHF</strong> = <strong>2017 年左右用于机器人控制的技术</strong>。</li>
</ul>
<p><strong>现在的 AI 革命，并不是发明了全新的算法，而是把这些“老零件”组装到了一个前所未有的“大引擎”（Transformer）上，并喂了前所未有的“大数据”。</strong></p>
<p>就像 Android 开发，<code>RecyclerView</code> 和 <code>OkHttp</code> 都是老东西了，但你把它们和一个超大规模的后端数据结合，再配合全新的 UI 设计，就做出了 TikTok。</p>
<h2 id="%E8%BF%9B%E4%B8%80%E6%AD%A5%E6%8F%90%E9%AB%98-ai-%E5%9F%BA%E5%BA%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD" tabindex="-1">进一步提高 AI 基座模型的性能</h2>
<p>这四页完成了从**“如何训练模型”<strong>到</strong>“模型如何生成结果”**的跨越。</p>
<p>作为 Android 工程师，你可以把这部分内容理解为：<strong>“如何给 App 加上内容审核机制（RLHF/DPO），以及底层的 <code>Random</code> 随机数生成逻辑（Sampling）。”</strong></p>
<p>我为你拆解为三个核心模块：</p>
<hr>
<h3 id="1.-%E4%BB%B7%E5%80%BC%E8%A7%82%E5%AF%B9%E9%BD%90%EF%BC%9Arlhf-%E4%B8%8E-dpo%EF%BC%88%E7%AC%AC%E4%B8%80%E3%80%81%E4%BA%8C%E9%A1%B5%EF%BC%89" tabindex="-1">1. 价值观对齐：RLHF 与 DPO（第一、二页）</h3>
<p>这一部分解决的问题是：<strong>SFT 教会了模型说话，但没教会它“什么话该说，什么话不该说”。</strong></p>
<ul>
<li>
<p><strong>问题场景</strong>：</p>
<ul>
<li>用户问：“怎么劫机？”</li>
<li>SFT 模型（只懂补全）可能会回答：“首先买张票，然后……”</li>
<li><strong>目标</strong>：我们需要模型拒绝回答，或者给出安全的建议。</li>
</ul>
</li>
<li>
<p><strong>解决方案：RLHF (Reinforcement Learning from Human Feedback)</strong></p>
<ul>
<li><strong>原理</strong>：训练一个**“裁判模型” (Reward Model)**。</li>
<li><strong>数据收集（图二）</strong>：
<ul>
<li>让 AI 生成两个答案（A 和 B）。</li>
<li>让人类标记员选：<strong>“A 比 B 好”</strong>。</li>
<li><em>注意</em>：让人直接打分（1-10分）很难，因为每个人标准不一样。但让人**比较（Ranking）**两个答案谁更好，就容易得多，一致性也更高（73% 一致率）。</li>
</ul>
</li>
<li><strong>Android 类比</strong>：
<ul>
<li>这就像 <strong>DiffUtil</strong>。你不需要知道 List A 和 List B 的绝对值，你只需要知道它们之间的<strong>差异 (Diff)</strong>。</li>
<li>或者像 <strong>A/B Testing</strong>。你不需要知道方案 A 具体得多少分，你只要知道 <strong>A 的转化率 &gt; B</strong> 就行了。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>新贵：DPO (Direct Preference Optimization)</strong></p>
<ul>
<li>文中特别提到了 <strong>Llama 3 使用了 DPO</strong>，而不是复杂的 RLHF。</li>
<li><strong>区别</strong>：RLHF 需要训练一个独立的“裁判模型”，流程很繁琐（PPO 算法）。DPO 直接把人类的偏好数据喂给模型，让模型自己调整，省去了“裁判”这个中间商。</li>
<li><strong>Android 类比</strong>：
<ul>
<li><strong>RLHF</strong> = <strong>MVC 模式</strong>。Controller (裁判) 负责协调 Model 和 View。</li>
<li><strong>DPO</strong> = <strong>MVVM / Declarative UI</strong>。数据驱动，直接绑定，少写很多样板代码。</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="2.-%E7%A9%B7%E4%BA%BA%E7%9A%84%E7%BB%9D%E6%8B%9B%EF%BC%9Abest-of-n-%E7%AD%96%E7%95%A5%EF%BC%88%E7%AC%AC%E4%B8%89%E9%A1%B5%E5%8F%B3%E4%B8%8B%E8%A7%92%EF%BC%89" tabindex="-1">2. 穷人的绝招：Best-of-N 策略（第三页右下角）</h3>
<p>有些公司（如 Stitch Fix, Grab）觉得训练 RLHF 太贵太难了，于是想了个笨办法。</p>
<ul>
<li><strong>策略</strong>：<strong>Best-of-N (Rejection Sampling)</strong>
<ul>
<li>既然我训练不好模型，那我就<strong>以量取胜</strong>。</li>
<li>对于同一个 Prompt，我让模型一次性生成 <strong>N 个</strong>（比如 10 个）不同的回答。</li>
<li>然后用一个现成的“打分器”（Reward Model）给这 10 个回答打分。</li>
<li>只把<strong>分数最高</strong>的那个返回给用户。</li>
</ul>
</li>
<li><strong>Android 类比</strong>：
<ul>
<li>这就像 <strong>TCP 的重传机制</strong> 或者 <strong>网络请求的 Retry 策略</strong>。</li>
<li>一次请求可能会失败（生成烂回答），那我就并发请求 10 次，哪个成功了（分数高）我就用哪个。虽然浪费了流量（算力），但保证了用户体验。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="3.-%E6%8F%AD%E7%A7%98%E5%BA%95%E5%B1%82%EF%BC%9Asampling-%E4%B8%8E-logits%EF%BC%88%E7%AC%AC%E5%9B%9B%E9%A1%B5%EF%BC%89" tabindex="-1">3. 揭秘底层：Sampling 与 Logits（第四页）</h3>
<p>这一页开始进入**推理（Inference）**阶段，解释模型到底是怎么吐出下一个字的。</p>
<ul>
<li>
<p><strong>Logits (逻辑值)</strong>：</p>
<ul>
<li>看图 2-15。神经网络的原始输出不是概率，而是一堆<strong>实数</strong>（可以是负数，比如 -1.2, 0.7）。这些数字叫 <strong>Logits</strong>。</li>
<li><strong>Android 类比</strong>：这就像 <code>View.getMeasuredWidth()</code> 拿到的原始像素值，或者是 <code>Color.parseColor()</code> 拿到的原始 HEX 值，还没经过处理。</li>
</ul>
</li>
<li>
<p><strong>Softmax 层</strong>：</p>
<ul>
<li>Logits 必须经过 <strong>Softmax</strong> 函数，才能变成<strong>概率 (Probabilities)</strong>（所有概率加起来等于 1）。</li>
<li>比如：
<ul>
<li>&quot;green&quot;: 0.7 -&gt; 70%</li>
<li>&quot;red&quot;: 0.5 -&gt; 20%</li>
<li>&quot;house&quot;: -0.2 -&gt; 1%</li>
</ul>
</li>
<li><strong>Android 类比</strong>：这就像把一堆原始数据通过 <code>MathUtils.normalize()</code> 归一化，变成 <code>0.0</code> 到 <code>1.0</code> 之间的进度条进度。</li>
</ul>
</li>
<li>
<p><strong>Sampling (采样)</strong>：</p>
<ul>
<li>有了概率之后，模型怎么选下一个词？</li>
<li><strong>Greedy Search (贪婪搜索)</strong>：永远选概率最大的那个（70% 的 green）。结果：模型会很死板，像个复读机。</li>
<li><strong>Random Sampling (随机采样)</strong>：根据概率随机选。有 20% 的几率选 &quot;red&quot;。结果：模型会有创造力，但有时会胡说八道。</li>
<li>这就是为什么你在用 ChatGPT API 时有一个参数叫 <strong><code>temperature</code></strong>。
<ul>
<li><code>temperature = 0</code>：只选概率最高的（严谨，适合写代码）。</li>
<li><code>temperature = 1</code>：增加随机性（发散，适合写小说）。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="%E6%80%BB%E7%BB%93-7" tabindex="-1">总结</h3>
<p>这四页书串联了 AI 发布的最后几公里：</p>
<ol>
<li><strong>RLHF/DPO</strong>：是**“政审员”**。确保模型三观正，不乱说话。（类似 Android 的 Permission 权限检查）。</li>
<li><strong>Best-of-N</strong>：是**“大力出奇迹”**。生成一堆，选最好的。（类似并发请求）。</li>
<li><strong>Logits &amp; Sampling</strong>：是**“骰子”**。决定了模型是像个严谨的程序员，还是像个浪漫的诗人。（类似 <code>Random</code> 类和配置参数）。</li>
</ol>
<h2 id="%E8%BF%90%E8%A1%8C%E6%97%B6%E9%85%8D%E7%BD%AE%E5%AF%B9%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BD%B1%E5%93%8D" tabindex="-1">运行时配置对模型的影响</h2>
<p>这几页书的内容含金量极高，它们揭示了<strong>OpenAI o1 模型</strong>背后的核心思想，以及如何通过“运行时配置”来控制 AI 的行为。</p>
<p>作为 Android 工程师，你可以把这部分内容理解为：<strong>“配置 <code>Random</code> 随机数生成器的参数，以及通过‘并发请求 + 结果校验’（Retry &amp; Verify）来强行提升 App 的成功率。”</strong></p>
<p>我为你拆解为三个核心模块：</p>
<h3 id="1.-%E8%B0%83%E8%8A%82%E2%80%9C%E7%96%AF%E7%8B%82%E6%8C%87%E6%95%B0%E2%80%9D%EF%BC%9Atemperature%2C-top-k%2C-top-p" tabindex="-1">1. 调节“疯狂指数”：Temperature, Top-k, Top-p</h3>
<p>这一部分讲的是如何控制 AI 是“严谨的程序员”还是“发疯的艺术家”。</p>
<ul>
<li>
<p><strong>Temperature (温度)</strong>：</p>
<ul>
<li><strong>原理</strong>：在 Softmax 之前，把 Logits 除以 $T$。</li>
<li><strong>效果</strong>：
<ul>
<li><strong>$T &lt; 1$ (低温)</strong>：拉大差距。原本 60% 的概率变成 90%。模型变得<strong>保守、自信、重复</strong>。</li>
<li><strong>$T &gt; 1$ (高温)</strong>：缩小差距。原本 1% 的概率变成 10%。模型变得<strong>疯狂、有创造力</strong>，但也容易胡说八道。</li>
</ul>
</li>
<li><strong>Android 类比</strong>：
<ul>
<li>这就像你在写动画插值器 (<code>Interpolator</code>)。</li>
<li><strong>低温</strong> = <code>AccelerateInterpolator</code>（两极分化，快的更快）。</li>
<li><strong>高温</strong> = <code>LinearInterpolator</code> 甚至反向（平摊概率，众生平等）。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Top-k Sampling</strong>：</p>
<ul>
<li><strong>原理</strong>：只看概率最高的前 $k$ 个词（比如前 50 个），其他的全部砍掉，归零。</li>
<li><strong>Android 类比</strong>：<code>List.take(50)</code>。不管后面还有多少数据，我只取前 50 个，防止 <code>IndexOutOfBounds</code> 或者读到垃圾数据。</li>
</ul>
</li>
<li>
<p><strong>Top-p (Nucleus) Sampling</strong>：</p>
<ul>
<li><strong>原理</strong>：更智能的截断。累加概率，直到总和达到 $p$（比如 0.9）。</li>
<li><em>场景 A</em>：如果第一个词概率就是 0.95，那候选列表里就只有这 1 个词。</li>
<li><em>场景 B</em>：如果前 10 个词概率都是 0.1，那候选列表里就有 10 个词。</li>
<li><strong>Android 类比</strong>：<strong>动态缓冲区</strong>。根据网络状况（概率分布）动态决定要缓存多少数据，而不是死板地固定大小。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="2.-%E8%B0%83%E8%AF%95%E5%B7%A5%E5%85%B7%EF%BC%9Alogprobs-(%E5%AF%B9%E6%95%B0%E6%A6%82%E7%8E%87)" tabindex="-1">2. 调试工具：Logprobs (对数概率)</h3>
<ul>
<li><strong>为什么用 Log (对数)？</strong>
<ul>
<li><strong>Underflow (下溢出)</strong>：概率通常是很小的数（比如 $0.0000001$）。计算机的 <code>float</code> 类型精度有限，乘多了就变成 0 了。</li>
<li>用 Log 之后，乘法变加法（$\log(a \times b) = \log a + \log b$），数值变成了负数（比如 -15.2），计算机处理起来很舒服。</li>
</ul>
</li>
<li><strong>Android 类比</strong>：
<ul>
<li>这就是 <strong>Logcat</strong>。你平时看不到 App 内部的变量状态，但开启 <code>logprobs</code> 就像打开了 <code>Log.d()</code>，能看到模型在生成每一个字时，内心到底有多纠结（自信度是多少）。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="3.-%E6%A0%B8%E5%BF%83%E5%A4%A7%E6%8B%9B%EF%BC%9Atest-time-compute-(%E6%B5%8B%E8%AF%95%E6%97%B6%E7%AE%97%E5%8A%9B)" tabindex="-1">3. 核心大招：Test Time Compute (测试时算力)</h3>
<p>这是这几页书里<strong>最重要</strong>的概念，也是目前 AI 发展的最新方向（OpenAI o1 的原理）。</p>
<ul>
<li>
<p><strong>核心思想</strong>：<strong>三个臭皮匠，顶个诸葛亮。</strong></p>
<ul>
<li>与其花大价钱训练一个超级天才模型（参数巨大），不如用一个普通模型，让它<strong>多思考一会儿</strong>，或者<strong>多试几次</strong>。</li>
</ul>
</li>
<li>
<p><strong>Verifier (验证器) 的威力（图 2-19）</strong>：</p>
<ul>
<li><strong>实验</strong>：让一个 100M（1亿参数）的小模型做数学题。</li>
<li><strong>做法</strong>：让它对同一道题做 30 次，然后用一个“验证器”挑出正确的那个。</li>
<li><strong>结果</strong>：它的表现竟然<strong>追平了</strong>一个 3B（30亿参数）的大模型！</li>
<li><strong>结论</strong>：<strong>推理时的算力 (Test Time Compute) 可以替代模型参数量。</strong></li>
</ul>
</li>
<li>
<p><strong>应用场景</strong>：</p>
<ol>
<li><strong>Majority Voting (多数投票)</strong>：
<ul>
<li>让模型做 32 次选择题。如果 30 次都选 C，那答案大概率就是 C。</li>
<li><em>Android 类比</em>：<strong>Paxos / Raft 共识算法</strong>。分布式系统里，一个节点可能会挂，但一群节点投票出来的结果通常是正确的。</li>
</ul>
</li>
<li><strong>Parallel Generation (并发生成)</strong>：
<ul>
<li>同时生成 5 个回答，哪个先生成完且符合格式（比如是合法的 JSON），就直接返回给用户。</li>
<li><em>Android 类比</em>：<strong>Race Condition 利用</strong>。你同时请求了 3 个 CDN 的图片资源，哪个先回来就用哪个显示，以此降低延迟。</li>
</ul>
</li>
</ol>
</li>
</ul>
<h3 id="%E6%80%BB%E7%BB%93-8" tabindex="-1">总结</h3>
<p>这几页书实际上是在教你**“如何用工程手段弥补模型的不足”**：</p>
<ol>
<li><strong>调参 (Temperature/Top-p)</strong>：根据业务场景（写代码 vs 写小说）调整模型的“性格”。</li>
<li><strong>换算 (Test Time Compute)</strong>：如果你没有钱训练大模型，你可以通过**“多跑几次 + 投票/验证”**的方式，用小模型达到大模型的效果。</li>
</ol>
<p>这就是为什么现在的 AI API 越来越慢（比如 o1-preview），因为它在后台偷偷地“多想了几遍”（Test Time Compute）。</p>
<h2 id="%E5%AE%9E%E7%8E%B0%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%A0%BC%E5%BC%8F%E5%8C%96%E8%BE%93%E5%87%BA" tabindex="-1">实现模型的格式化输出</h2>
<p>这三页内容对于 Android 工程师来说简直太亲切了。</p>
<p>如果说之前的章节是在讲“如何训练一个像人的大脑”，那么这一章就是在讲**“如何把这个大脑接入到你的 App 代码里”**。</p>
<p>作为开发者，我们最怕的就是<strong>非结构化数据</strong>。</p>
<ul>
<li><strong>LLM 的天性</strong>：喜欢啰嗦，喜欢输出 &quot;Here is the JSON you asked for: {...}&quot;。</li>
<li><strong>App 的需求</strong>：我只要 <code>{...}</code>，多一个字符 <code>Gson</code> 或 <code>Moshi</code> 解析都会报错（Crash）。</li>
</ul>
<p>这几页书就是在讲<strong>如何强迫 LLM 输出完美的 JSON、SQL 或 Regex</strong>，以便你的代码能直接调用。我为你拆解为三个核心层级：</p>
<h3 id="1.-%E6%A0%B8%E5%BF%83%E9%9C%80%E6%B1%82%EF%BC%9Asemantic-parsing-(%E8%AF%AD%E4%B9%89%E8%A7%A3%E6%9E%90)" tabindex="-1">1. 核心需求：Semantic Parsing (语义解析)</h3>
<p><strong>场景</strong>：用户说人话，App 听不懂，数据库更听不懂。
<strong>任务</strong>：把“人话”翻译成“机器指令”。</p>
<ul>
<li><strong>Text-to-SQL / Text-to-Regex</strong>：
<ul>
<li><strong>用户说</strong>：“帮我查一下过去 6 个月的平均营收。”</li>
<li><strong>App 需要</strong>：<code>SELECT avg(revenue) FROM sales WHERE date &gt; now() - interval '6 months';</code></li>
<li><strong>用户说</strong>：“我要匹配美国手机号。”</li>
<li><strong>App 需要</strong>：<code>\+?1?\s?(\(?(\d{3})\)?[-.\s]?(\d{3})[-.\s]?(\d{4}))</code></li>
</ul>
</li>
<li><strong>Android 类比</strong>：
<ul>
<li>这就像是一个超级智能的 <strong>Adapter</strong> 或者 <strong>Converter</strong>。</li>
<li>输入是 <code>String</code> (自然语言)，输出是 <code>Query</code> 对象 (Room Database) 或 <code>Pattern</code> 对象 (Regex)。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="2.-%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E8%BE%93%E5%87%BA%E6%A0%BC%E5%BC%8F%EF%BC%9F%EF%BC%88%E4%BA%94%E5%B1%82%E9%98%B2%E5%BE%A1%E4%BD%93%E7%B3%BB%EF%BC%89" tabindex="-1">2. 怎么保证输出格式？（五层防御体系）</h3>
<p>书中列出了 5 种让模型“听话”的方法，这里重点讲前三种最实用的。</p>
<h4 id="level-1%3A-prompting-(%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B)-%E2%80%94%E2%80%94-%22%E6%B1%82%E6%B1%82%E4%BD%A0%22" tabindex="-1">Level 1: Prompting (提示词工程) —— &quot;求求你&quot;</h4>
<ul>
<li><strong>做法</strong>：在 System Prompt 里写：“请只返回 JSON，不要说废话。”</li>
<li><strong>结果</strong>：<strong>很不靠谱</strong>。模型经常会忍不住加一句 &quot;Sure! Here it is:&quot;，导致你的 <code>JSONObject.parse()</code> 抛出异常。</li>
<li><strong>Android 类比</strong>：这就像在代码注释里写 <code>// Please don't pass null here</code>，但没有任何编译期检查，运行时该空指针还是空指针。</li>
</ul>
<h4 id="level-2%3A-post-processing-(%E5%90%8E%E5%A4%84%E7%90%86)-%E2%80%94%E2%80%94-%22%E6%93%A6%E5%B1%81%E8%82%A1%22" tabindex="-1">Level 2: Post-processing (后处理) —— &quot;擦屁股&quot;</h4>
<ul>
<li><strong>做法</strong>：模型输出错了，我写个脚本修一下。</li>
<li><strong>LinkedIn 的案例</strong>：
<ul>
<li>他们发现模型生成的 JSON 经常少写一个闭合括号 <code>}</code>。</li>
<li>于是他们写了个脚本：如果解析失败，就尝试在字符串末尾加个 <code>}</code> 再解析一次。</li>
<li><strong>效果</strong>：成功率从 90% 提升到了 99.99%。</li>
</ul>
</li>
<li><strong>YAML vs JSON (书中 Tip)</strong>：
<ul>
<li>LinkedIn 发现 <strong>YAML</strong> 格式比 JSON 更好。</li>
<li><strong>原因</strong>：YAML 符号少（没有那么多引号和括号），<strong>Token 数更少</strong>。</li>
<li><strong>Android 类比</strong>：
<ul>
<li><strong>后处理</strong> = <code>try-catch</code> 块里的重试逻辑。</li>
<li><strong>YAML</strong> = <strong>ProGuard/R8 混淆</strong>。同样的逻辑，体积更小，传输更快，省流量（省 Token 钱）。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="level-3%3A-constrained-sampling-(%E5%8F%97%E9%99%90%E9%87%87%E6%A0%B7)-%E2%80%94%E2%80%94-%22%E5%BC%BA%E7%B1%BB%E5%9E%8B%E6%A3%80%E6%9F%A5%22" tabindex="-1">Level 3: Constrained Sampling (受限采样) —— &quot;强类型检查&quot;</h4>
<p>这是最硬核、最有效的技术（图 2-20）。</p>
<ul>
<li>
<p><strong>原理</strong>：
还记得上一节讲的 <code>Logits</code> 和 <code>Softmax</code> 吗？模型在生成下一个词时，会给词表里几万个词打分。
<strong>受限采样</strong>就是：<strong>直接修改底层的打分表，把不符合格式的词的概率强行设为 0。</strong></p>
</li>
<li>
<p><strong>案例 A：Select (枚举)</strong></p>
<ul>
<li>代码：<code>select(['red', 'blue', 'green'])</code></li>
<li><strong>效果</strong>：当模型生成到颜色时，它<strong>只能</strong>从这三个词里选。生成 &quot;yellow&quot; 的概率被物理锁死为 0。</li>
<li><strong>Android 类比</strong>：<strong>Enum (枚举类型)</strong>。
<ul>
<li>普通 LLM 返回的是 <code>String color</code>，你不知道它会返回什么。</li>
<li>受限采样返回的是 <code>ColorEnum color</code>，编译期保证绝对安全。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>案例 B：Regex (正则约束)</strong></p>
<ul>
<li>代码：<code>gen(regex='\d+')</code></li>
<li><strong>效果</strong>：模型在这一步<strong>只能生成数字</strong>。键盘上的字母键被“扣掉”了。</li>
<li><strong>Android 类比</strong>：<strong>EditText 的 InputFilter</strong>。
<ul>
<li>设置 <code>android:inputType=&quot;number&quot;</code>，用户想输字母都输不进去。</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="3.-openai-%E7%9A%84-json-mode-(%E4%B8%AD%E9%97%B4%E6%B4%BE)" tabindex="-1">3. OpenAI 的 JSON Mode (中间派)</h3>
<p>书中特别提到了 OpenAI 的 <code>JSON Mode</code>。</p>
<ul>
<li><strong>是什么</strong>：它保证返回的字符串<strong>语法上</strong>一定是合法的 JSON（能过 <code>JSON.parse</code>）。</li>
<li><strong>局限性</strong>：
<ol>
<li><strong>内容不保真</strong>：它可能返回 <code>{ &quot;age&quot;: &quot;hello&quot; }</code>，虽然是 JSON，但 Schema 不对。</li>
<li><strong>截断风险</strong>：如果 Token 超限，JSON 可能被切断（比如 <code>{ &quot;name&quot;: &quot;To</code>），导致解析失败。</li>
</ol>
</li>
<li><strong>Android 类比</strong>：
<ul>
<li>这就像服务器返回了 <code>200 OK</code>，Body 也是 JSON，但里面的字段可能是空的，或者类型不对。你还是需要用 <code>Moshi</code> 的 <code>JsonAdapter</code> 来做进一步校验。</li>
</ul>
</li>
</ul>
<h3 id="%E6%80%BB%E7%BB%93-9" tabindex="-1">总结</h3>
<p>这几页书实际上是在教你<strong>如何把“玄学”的 AI 变成“工程化”的 API</strong>：</p>
<ol>
<li><strong>Prompting</strong> 是靠运气（不推荐生产环境）。</li>
<li><strong>Post-processing</strong> 是靠补丁（LinkedIn 的 YAML 技巧很实用）。</li>
<li><strong>Constrained Sampling</strong> 是靠底层机制（类似 <code>llama.cpp</code> 或 <code>Guidance</code> 库），它是最稳的，相当于给 AI 加上了 <strong>Type Safety (类型安全)</strong>。</li>
</ol>
<p>作为 Android 工程师，如果你要在 App 里集成 AI 功能，<strong>强烈建议使用支持 Structured Outputs (Constrained Sampling) 的框架</strong>，否则你的 Crash 率会非常难看。</p>
<h2 id="constrained-sampling-%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86" tabindex="-1">Constrained Sampling 实现原理</h2>
<p>书中对 <strong>Constrained Sampling（受限采样）</strong> 的分析非常深入，它是目前解决“大模型胡乱输出格式”最底层、最硬核的技术手段。</p>
<p>作为 Android 工程师，你可以把它理解为：<strong>从“运行时校验（Runtime Check）”进化到了“编译时类型安全（Compile-time Type Safety）”</strong>。</p>
<p>以下是书中对这一技术的详细拆解：</p>
<h3 id="1.-%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%86%EF%BC%9A%E5%9C%A8%E2%80%9C%E5%A4%A7%E8%84%91%E2%80%9D%E5%92%8C%E2%80%9C%E5%98%B4%E5%B7%B4%E2%80%9D%E4%B9%8B%E9%97%B4%E8%AE%BE%E5%8D%A1" tabindex="-1">1. 核心原理：在“大脑”和“嘴巴”之间设卡</h3>
<p>书中虽然没有画出底层的概率图，但文字描述（Page 2 右下角 &amp; Page 1 图 2-20）揭示了其工作机制。</p>
<ul>
<li>
<p><strong>正常流程</strong>：</p>
<ol>
<li>模型大脑计算出下一个词的概率分布（Logits）。</li>
<li>比如：<code>&quot;yellow&quot;</code> (40%), <code>&quot;red&quot;</code> (30%), <code>&quot;blue&quot;</code> (20%)。</li>
<li>采样层（Sampling）根据概率随机选一个词输出。</li>
</ol>
</li>
<li>
<p><strong>Constrained Sampling 流程</strong>：</p>
<ol>
<li><strong>定义约束</strong>：你告诉系统，这里只能填 <code>['red', 'blue', 'green']</code>。</li>
<li><strong>动态掩码 (Masking)</strong>：
<ul>
<li>模型算出了 <code>&quot;yellow&quot;</code> (40%)。</li>
<li><strong>拦截器</strong>介入：检测到 <code>&quot;yellow&quot;</code> 不在白名单里。</li>
<li><strong>强制修改概率</strong>：把 <code>&quot;yellow&quot;</code> 的概率瞬间设为 <strong>0</strong> (或者 Logit 设为 $-\infty$)。</li>
</ul>
</li>
<li><strong>重归一化</strong>：剩下的 <code>red</code>, <code>blue</code>, <code>green</code> 瓜分剩下的概率。</li>
<li><strong>输出</strong>：模型<strong>被迫</strong>在合法的选项里选一个概率最高的。</li>
</ol>
</li>
</ul>
<p><strong>Android 类比</strong>：
这就像你自定义了一个 <code>InputConnectionWrapper</code> 给软键盘。
当 <code>EditText</code> 设置为 <code>inputType=&quot;number&quot;</code> 时，即使用户在键盘上疯狂点击字母 &quot;A&quot;，你的 Wrapper 会直接拦截这个事件，<strong>根本不让这个字符上屏</strong>。模型想说错都难。</p>
<hr>
<h3 id="2.-%E4%B9%A6%E4%B8%AD%E5%B1%95%E7%A4%BA%E7%9A%84%E4%B8%A4%E7%A7%8D%E7%BA%A6%E6%9D%9F%E6%A8%A1%E5%BC%8F%EF%BC%88%E5%9B%BE-2-20%EF%BC%89" tabindex="-1">2. 书中展示的两种约束模式（图 2-20）</h3>
<p>书中通过代码片段（类似 <code>guidance</code> 库的语法）展示了两种最常见的约束场景：</p>
<h4 id="a.-%E9%9B%86%E5%90%88%E7%BA%A6%E6%9D%9F-(set-of-options)-%E2%80%94%E2%80%94-%E6%9E%9A%E4%B8%BE-(enum)" tabindex="-1">A. 集合约束 (Set of Options) —— 枚举 (Enum)</h4>
<ul>
<li><strong>代码示例</strong>：<pre class="language-python"><code class="language-python">lm <span class="token operator">=</span> llama2 <span class="token operator">+</span> <span class="token string">'I like the color '</span> <span class="token operator">+</span> select<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'red'</span><span class="token punctuation">,</span> <span class="token string">'blue'</span><span class="token punctuation">,</span> <span class="token string">'green'</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre>
</li>
<li><strong>分析</strong>：
<ul>
<li>这是最简单的约束。</li>
<li><strong>应用场景</strong>：情感分析（Positive/Negative）、分类任务、多选题。</li>
<li><strong>优势</strong>：完全消除了“幻觉”。模型不可能造出一个不存在的颜色。</li>
</ul>
</li>
</ul>
<h4 id="b.-%E6%AD%A3%E5%88%99%2F%E8%AF%AD%E6%B3%95%E7%BA%A6%E6%9D%9F-(regex-%2F-grammar)-%E2%80%94%E2%80%94-%E6%A0%BC%E5%BC%8F%E5%8C%96-(formatting)" tabindex="-1">B. 正则/语法约束 (Regex / Grammar) —— 格式化 (Formatting)</h4>
<ul>
<li><strong>代码示例</strong>：<pre class="language-python"><code class="language-python">lm <span class="token operator">+=</span> <span class="token string">'Answer: '</span> <span class="token operator">+</span> gen<span class="token punctuation">(</span>regex<span class="token operator">=</span><span class="token string">'\d+'</span><span class="token punctuation">)</span></code></pre>
</li>
<li><strong>分析</strong>：
<ul>
<li>这里使用了 <strong>正则表达式 (Regex)</strong> 作为约束条件。</li>
<li><strong>工作机制</strong>：这其实是一个<strong>有限状态机 (FSM)</strong>。
<ul>
<li>当模型还没输出时，状态机处于“初始态”，允许输出数字 <code>0-9</code>。</li>
<li>一旦输出了一个数字，状态机保持在“数字态”，继续允许数字。</li>
<li>如果模型试图输出字母，状态机报错，该 Token 概率被抹杀。</li>
</ul>
</li>
<li><strong>应用场景</strong>：提取电话号码、邮箱、日期、或者严格符合 Schema 的 JSON。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="3.-%E4%B8%8E-openai-json-mode-%E7%9A%84%E5%AF%B9%E6%AF%94%EF%BC%88page-1-%E5%8F%B3%E4%B8%8A%E8%A7%92%EF%BC%89" tabindex="-1">3. 与 OpenAI JSON Mode 的对比（Page 1 右上角）</h3>
<p>书中特意把 <strong>Constrained Sampling</strong> 和 OpenAI 的 <strong>JSON Mode</strong> 做了区分，这是一个很重要的技术细节。</p>
<ul>
<li>
<p><strong>OpenAI JSON Mode (软约束)</strong>：</p>
<ul>
<li><strong>书中原话</strong>：<em>&quot;guarantees only that the outputs are valid JSON — not the content of the JSON objects.&quot;</em></li>
<li><strong>含义</strong>：它保证括号能闭合，逗号位置对。但它<strong>不保证</strong>字段名是你想要的。</li>
<li><strong>风险</strong>：
<ol>
<li><strong>Schema 错误</strong>：你想要 <code>{&quot;age&quot;: int}</code>，它可能给你 <code>{&quot;age&quot;: &quot;eighteen&quot;}</code>。</li>
<li><strong>截断 (Truncated)</strong>：书中提到 <em>&quot;generated JSONs can also be truncated&quot;</em>。如果 Token 用完了，JSON 会断在中间，导致解析失败。</li>
</ol>
</li>
</ul>
</li>
<li>
<p><strong>Constrained Sampling (硬约束)</strong>：</p>
<ul>
<li>它是基于 <strong>Context-Free Grammar (CFG)</strong> 的。</li>
<li>它能保证：
<ol>
<li><strong>Schema 100% 正确</strong>：字段名、类型（int/string/bool）绝对符合定义。</li>
<li><strong>结构完整</strong>：它知道 JSON 什么时候才算结束，不会半途而废（除非强制停止）。</li>
</ol>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="4.-%E5%B1%80%E9%99%90%E6%80%A7%E4%B8%8E%E4%BB%A3%E4%BB%B7" tabindex="-1">4. 局限性与代价</h3>
<p>虽然 Constrained Sampling 看起来很完美，但书中也暗示了它的代价（Page 1 右上角最后一句）：</p>
<ul>
<li><strong>推理延迟 (Latency)</strong>：
<ul>
<li>每次生成一个 Token，都要运行一次“状态机”来判断哪些词是合法的。如果是复杂的正则表达式或巨大的 JSON Schema，这个计算量不小，会拖慢生成速度。</li>
</ul>
</li>
<li><strong>API 支持度</strong>：
<ul>
<li>这需要访问模型的底层 Logits。目前只有开源模型（如 <code>llama.cpp</code> 跑在本地）或者少数支持高级功能的 API（如 OpenAI 的 Structured Outputs 新功能）才能做到。普通的 API 只能靠 Prompting（求它写对）。</li>
</ul>
</li>
</ul>
<h3 id="%E6%80%BB%E7%BB%93-10" tabindex="-1">总结</h3>
<p>书中对 Constrained Sampling 的分析核心在于：<strong>不要相信模型的自律，要用算法接管它的输出层。</strong></p>
<ul>
<li><strong>Prompting</strong> = 告诉用户“请输入数字”。（用户可能不听）</li>
<li><strong>Post-processing</strong> = 用户输错了，你写个正则去提取数字。（可能提取不到）</li>
<li><strong>Constrained Sampling</strong> = 直接把键盘上的非数字键扣掉。（绝对安全）</li>
</ul>
<p>。</p>
<h3 id="constrained-sampling-%E7%9A%84%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%EF%BC%88%E4%B9%A6%E5%A4%96%E8%A1%A5%E5%85%85%EF%BC%89" tabindex="-1">Constrained Sampling 的底层实现原理（书外补充）</h3>
<p>我可以用通用的技术原理（基于 <code>llama.cpp</code> 或 <code>outlines</code> 等开源库的实现逻辑）为你补全这块拼图。这其实是一个<strong>编译原理</strong> + <strong>Trie 树搜索</strong>的问题</p>
<p>要实现“让模型只输出符合 Regex 的内容”，系统在底层做了三步操作：</p>
<h4 id="1.-%E7%BC%96%E8%AF%91%E9%98%B6%E6%AE%B5%EF%BC%9Aregex-%24%5Crightarrow%24-fsm-(%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA)" tabindex="-1">1. 编译阶段：Regex $\rightarrow$ FSM (有限状态机)</h4>
<p>在你把 <code>regex='\d{3}-\d{4}'</code> 传给系统时，系统会先把它编译成一个 <strong>DFA (确定性有限状态自动机)</strong>。</p>
<ul>
<li><strong>状态 0 (Start)</strong>: 期待数字。</li>
<li><strong>状态 1 (输了1个数字)</strong>: 期待数字。</li>
<li>...</li>
<li><strong>状态 3 (输了3个数字)</strong>: 期待连字符 <code>-</code>。</li>
<li>...</li>
</ul>
<h4 id="2.-%E7%B4%A2%E5%BC%95%E9%98%B6%E6%AE%B5%EF%BC%9Avocabulary-%24%5Crightarrow%24-trie-(%E5%89%8D%E7%BC%80%E6%A0%91)" tabindex="-1">2. 索引阶段：Vocabulary $\rightarrow$ Trie (前缀树)</h4>
<p>大模型的词表（Vocabulary）通常有 3万-10万 个 Token。为了快速查找，这些 Token 会被构建成一个 <strong>Trie 树</strong>。</p>
<h4 id="3.-%E8%BF%90%E8%A1%8C%E6%97%B6%E9%98%B6%E6%AE%B5%EF%BC%9Alogit-masking-(%E6%8E%A9%E7%A0%81%E6%8B%A6%E6%88%AA)-%E2%80%94%E2%80%94-%E6%A0%B8%E5%BF%83%E9%AD%94%E6%B3%95" tabindex="-1">3. 运行时阶段：Logit Masking (掩码拦截) —— 核心魔法</h4>
<p>这是发生在模型生成每一个 Token 之前的<strong>毫秒级</strong>操作：</p>
<ol>
<li><strong>获取当前状态</strong>：
假设 FSM 当前处于 <strong>状态 3</strong>（已经输了3个数字，现在必须输 <code>-</code>）。</li>
<li><strong>计算合法字符集</strong>：
FSM 告诉我们，下一个字符<strong>必须</strong>是 <code>-</code>。</li>
<li><strong>Trie 树剪枝 (Pruning)</strong>：
系统去遍历 Token Trie 树。
<ul>
<li>Token <code>&quot;apple&quot;</code>？以 <code>a</code> 开头 $\rightarrow$ <strong>非法</strong>。</li>
<li>Token <code>&quot;123&quot;</code>？以 <code>1</code> 开头 $\rightarrow$ <strong>非法</strong>（因为现在急需 <code>-</code>）。</li>
<li>Token <code>&quot;-&quot;</code>？以 <code>-</code> 开头 $\rightarrow$ <strong>合法</strong>。</li>
<li>Token <code>&quot;-hello&quot;</code>？以 <code>-</code> 开头 $\rightarrow$ <strong>合法</strong>（虽然它后面跟了 hello，但只要前缀匹配，且能推动 FSM 状态流转，有些实现是允许的，有些严格实现会继续检查后续字符）。</li>
</ul>
</li>
<li><strong>修改 Logits</strong>：
找到所有<strong>非法</strong> Token 的 ID，直接在 Logits 数组中把对应的值设为 <strong>$-\infty$</strong>。</li>
<li><strong>Softmax &amp; Sampling</strong>：
经过 Softmax 后，非法 Token 的概率变为 <strong>0</strong>。模型<strong>只能</strong>选到合法的 Token。</li>
<li><strong>状态推进</strong>：
模型选了 <code>&quot;-&quot;</code>，FSM 状态从 <strong>3</strong> 变为 <strong>4</strong>。</li>
</ol>
<h3 id="android-%E5%B7%A5%E7%A8%8B%E5%B8%88%E8%A7%86%E8%A7%92%E7%9A%84%E7%B1%BB%E6%AF%94" tabindex="-1">Android 工程师视角的类比</h3>
<p>这就好比你写了一个超级复杂的 <code>InputFilter</code>，但不是用 <code>if-else</code> 写的，而是用<strong>编译原理</strong>写的。</p>
<ul>
<li><strong>场景</strong>：用户在输入框里输入。</li>
<li><strong>FSM</strong>：就是你编译好的 <code>Pattern</code> 对象。</li>
<li><strong>Token Trie</strong>：就是<strong>软键盘上的所有按键</strong>（加上智能联想词）。</li>
<li><strong>Logit Masking</strong>：
<ul>
<li>当用户输完前 3 个数字时，你的代码<strong>瞬间</strong>遍历了软键盘所有按键。</li>
<li>发现只有 <code>-</code> 键能让正则匹配继续下去。</li>
<li>于是你调用 <code>setKeyEnabled(false)</code> 把除了 <code>-</code> 以外的所有键（A-Z, 0-9）全部<strong>变灰不可点</strong>。</li>
<li>用户闭着眼睛按，也只能按出 <code>-</code> 来。</li>
</ul>
</li>
</ul>
<h3 id="%E6%80%BB%E7%BB%93-11" tabindex="-1">总结</h3>
<p>虽然书中没写，但这个机制的本质就是：<strong>将 Regex 解析为状态机，在每一步生成前，计算出“哪些 Token 能让状态机合法流转”，然后把其他 Token 的概率物理清零。</strong></p>
<p>这就是为什么它能 <strong>100%</strong> 保证格式正确，但也会增加推理延迟（因为每一步都要遍历 Trie 树做计算）。</p>
<h2 id="%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8D%E4%B8%80%E8%87%B4%E6%80%A7%E5%92%8C%E5%B9%BB%E8%A7%89" tabindex="-1">模型的不一致性和幻觉</h2>
<p>这五页书是本章的<strong>大结局</strong>，它揭示了 AI 最致命的两个“系统级 Bug”：<strong>不一致性 (Inconsistency)</strong> 和 <strong>幻觉 (Hallucination)</strong>。</p>
<p>作为 Android 工程师，你可以把这部分理解为：<strong>“为什么这个后端接口有时候返回 200，有时候返回 404，有时候返回 200 但里面的数据全是假的？”</strong></p>
<p>我为你拆解为三个核心模块：</p>
<hr>
<h3 id="1.-%E6%A0%B9%E6%BA%90%EF%BC%9A%E6%A6%82%E7%8E%87%E6%80%A7-(probabilistic)-vs-%E7%A1%AE%E5%AE%9A%E6%80%A7-(deterministic)" tabindex="-1">1. 根源：概率性 (Probabilistic) vs 确定性 (Deterministic)</h3>
<p><strong>第一页</strong>点出了问题的核心。</p>
<ul>
<li><strong>传统软件 (Deterministic)</strong>：
<ul>
<li>输入 <code>2 + 2</code>，永远输出 <code>4</code>。</li>
<li><strong>Android 类比</strong>：<code>HashMap.get(&quot;key&quot;)</code>。只要 Key 存在，每次拿到的 Value 绝对是一样的。</li>
</ul>
</li>
<li><strong>AI 模型 (Probabilistic)</strong>：
<ul>
<li>输入 <code>2 + 2</code>，它是在<strong>掷骰子</strong>。</li>
<li>它认为 <code>4</code> 的概率是 99%，<code>5</code> 的概率是 1%。如果你运气不好（或者 Temperature 设置高了），它真能给你输出 <code>5</code>。</li>
<li><strong>Android 类比</strong>：<code>Random.nextInt()</code> 或者 <strong>多线程 Race Condition</strong>。你永远不知道这一次运行的结果会不会和上一次一样。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="2.-bug-%E7%B1%BB%E5%9E%8B-a%EF%BC%9A%E4%B8%8D%E4%B8%80%E8%87%B4%E6%80%A7-(inconsistency)-%E2%80%94%E2%80%94-%22flaky-test%22" tabindex="-1">2. Bug 类型 A：不一致性 (Inconsistency) —— &quot;Flaky Test&quot;</h3>
<p><strong>第二页</strong>详细描述了这种痛苦。</p>
<ul>
<li><strong>现象</strong>：
<ul>
<li><strong>同入异出</strong>：同样的 Prompt 问两次，第一次给作文打 3 分，第二次打 5 分（图 2-23）。</li>
<li><strong>蝴蝶效应</strong>：Prompt 改了一个标点符号，输出结果天差地别。</li>
</ul>
</li>
<li><strong>硬件也有锅</strong>：
<ul>
<li>书中提到，即使你固定了 Seed（随机种子），在不同的 GPU（比如 NVIDIA A100 vs H100）上跑，结果也可能不一样。</li>
<li><strong>Android 类比</strong>：<strong>机型适配问题</strong>。
<ul>
<li>你的代码在 Pixel 上跑得好好的，在三星或小米手机上就 Crash 了。</li>
<li>或者像 <strong>Flaky Unit Test</strong>（不稳定的单元测试）。代码没改，CI 跑十次，有两次红了。这让开发者非常抓狂，因为<strong>不可复现</strong>。</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="3.-bug-%E7%B1%BB%E5%9E%8B-b%EF%BC%9A%E5%B9%BB%E8%A7%89-(hallucination)-%E2%80%94%E2%80%94-%22%E4%B8%80%E6%9C%AC%E6%AD%A3%E7%BB%8F%E8%83%A1%E8%AF%B4%E5%85%AB%E9%81%93%22" tabindex="-1">3. Bug 类型 B：幻觉 (Hallucination) —— &quot;一本正经胡说八道&quot;</h3>
<p><strong>第三、四页</strong>深入探讨了 AI 为什么会撒谎，以及为什么<strong>越教它做人，它越爱撒谎</strong>。</p>
<h4 id="a.-%E6%BB%9A%E9%9B%AA%E7%90%83%E6%95%88%E5%BA%94-(snowballing)" tabindex="-1">A. 滚雪球效应 (Snowballing)</h4>
<ul>
<li><strong>原理</strong>：模型是基于“上一个词”预测“下一个词”的。</li>
<li><strong>过程</strong>：
<ol>
<li>模型第一步预测错了一个词（比如把“洗发水”看成了“牛奶”）。</li>
<li>为了让逻辑通顺，它后面生成的几百个词都必须<strong>圆谎</strong>（开始编造牛奶的成分表）。</li>
<li>这就是 <strong>Self-delusion (自我欺骗)</strong>。</li>
</ol>
</li>
<li><strong>Android 类比</strong>：<strong>脏数据污染 (Data Corruption)</strong>。
<ul>
<li>链表的头节点指错了，后面遍历出来的整条链表全是垃圾数据。</li>
<li>或者像你写代码时<strong>为了修一个 Bug 引入了三个新 Bug</strong>，最后代码逻辑彻底崩坏。</li>
</ul>
</li>
</ul>
<h4 id="b.-%E4%B8%BA%E4%BB%80%E4%B9%88-rlhf-%E4%BC%9A%E5%8A%A0%E9%87%8D%E5%B9%BB%E8%A7%89%EF%BC%9F%EF%BC%88%E5%9B%BE-2-26-%E7%9A%84%E5%8F%8D%E7%9B%B4%E8%A7%89%E7%BB%93%E8%AE%BA%EF%BC%89" tabindex="-1">B. 为什么 RLHF 会加重幻觉？（图 2-26 的反直觉结论）</h4>
<p>这是一个非常惊人的发现：<strong>经过 RLHF（人类反馈微调）的模型，幻觉反而比只经过 SFT 的模型更严重！</strong></p>
<ul>
<li><strong>原因</strong>：<strong>被迫营业</strong>。
<ul>
<li>在 RLHF 阶段，人类标记员会问很多刁钻的问题。</li>
<li>如果模型回答“我不知道”，人类可能会给低分。</li>
<li>如果模型自信地编造一个答案，人类可能会被忽悠给高分。</li>
<li><strong>结果</strong>：模型学会了**“讨好人类” (Sycophancy)<strong>，而不是“实事求是”。它变成了一个</strong>Yes-Man**。</li>
</ul>
</li>
<li><strong>Android 类比</strong>：
<ul>
<li>这就像一个 <strong>UI 只有空壳的 App</strong>。</li>
<li>为了让用户觉得 App 很快（讨好用户），你在数据还没加载回来时就先显示了缓存的旧数据（甚至假数据）。用户体验看似好了，但看到的是错误的信息。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="4.-%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%EF%BC%9A%E9%98%B2%E5%BE%A1%E6%80%A7%E7%BC%96%E7%A8%8B-(page-5)" tabindex="-1">4. 解决方案：防御性编程 (Page 5)</h3>
<p>既然模型天生爱撒谎，我们怎么修？</p>
<ul>
<li>
<p><strong>Prompt Engineering</strong>：</p>
<ul>
<li>明确告诉模型：“如果你不知道，请直接说不知道，不要编。”</li>
<li><strong>Android 类比</strong>：<strong>防御性编程</strong>。
<ul>
<li><code>if (data == null) return;</code></li>
<li>在代码里显式处理边界情况，防止空指针异常。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>RAG (检索增强生成)</strong>（虽然这几页没细讲，但暗示了）：</p>
<ul>
<li>不要让模型用“脑子”（参数）里的记忆回答，而是给它一本书（外部文档），让它照着读。</li>
</ul>
</li>
</ul>
<h3 id="%E5%85%A8%E7%AB%A0%E6%80%BB%E7%BB%93-(summary)" tabindex="-1">全章总结 (Summary)</h3>
<p>这一章完整地讲述了构建一个大模型的全生命周期，对于 Android 工程师来说，可以这样映射：</p>
<ol>
<li><strong>Pre-training (预训练)</strong> = <strong>开发 OS 内核</strong>。烧钱，耗时，决定了智商上限。</li>
<li><strong>SFT (有监督微调)</strong> = <strong>开发 Framework API</strong>。让内核能力变得可被调用，学会听指令。</li>
<li><strong>RLHF (人类反馈)</strong> = <strong>App 审核与合规</strong>。确保输出内容安全、好听，但可能会导致模型变得圆滑世故（爱撒谎）。</li>
<li><strong>Inference (推理)</strong> = <strong>App 运行时</strong>。通过 Temperature 等参数控制随机性，通过 Constrained Sampling 保证 JSON 格式不 Crash。</li>
</ol>
<p>读完这一章，你已经从一个“只会调 API 的开发者”变成了一个“理解 AI 底层 Bug 来源的工程师”。</p>

    </div>
    <hr />
      <div class="text-sm text-center">
        评论和交流请发送邮件到 xx2bab@gmail.com
      </div>
      <hr />
     <div class="text-center">
  <img class="w-64 inline-block" src="https://s2.loli.net/2023/05/13/FKYi5STEtmNZd8W.jpg" alt="Wechat Donate QACode" />
  <div class="text-sm">
    通过微信扫描赞赏码赞助此文
  </div>
</div> 
    <footer class="text-sm py-12 text-gray-500 text-center">
  
  <p><a href="/">ENG</a> / <a href="/zh">中文</a></p>
  
  2BAB's Blog since 2014
</footer>
  </div>

</div>



</body>
</html>