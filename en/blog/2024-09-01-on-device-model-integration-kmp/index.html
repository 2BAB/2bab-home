<html lang="en" dir="ltr">

<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta property="og:title" content="Adapting MediaPipe Demos for Kotlin Multiplatform: LLM Inference" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://2bab.me/en/blog/2024-09-01-on-device-model-integration-kmp/" />
  <link rel="stylesheet" href="/styles/main.css">
  <link rel="me" href="https://2bab.me/">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@1.27.0/themes/prism.min.css">
  
  <title>Adapting MediaPipe Demos for Kotlin Multiplatform: LLM Inference | 2BAB&#39;s Blog</title>
  
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NFRNXW3SHS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-NFRNXW3SHS');
</script>

<body>
  
<div>
  <div class="container mx-auto prose py-12 sm:py-24 px-12 sm:px-0">
    <div class="mb-12">
      <a class="no-underline font-bold" href="/">2BAB&#39;s Blog</a>
    </div>
    <h1>Adapting MediaPipe Demos for Kotlin Multiplatform: LLM Inference</h1>
    <div class="italic text-gray-500">
      2024/09/01
    </div>
    <div>
      <p>By reading this post, you'll learn about:</p>
<ol>
<li><strong>Porting the MediaPipe &quot;LLM Inference&quot; Android demo to Kotlin Multiplatform (KMP) to support iOS.</strong> Project repository: <a href="https://github.com/2BAB/mediapiper">https://github.com/2BAB/mediapiper</a></li>
<li><strong>Two common ways of calling iOS SDKs in KMP:</strong>
<ol>
<li><strong>Directly invoking third-party libraries added via Cocoapods in Kotlin.</strong></li>
<li><strong>Calling third-party libraries from the iOS project in Kotlin.</strong></li>
</ol>
</li>
<li><strong>Tips for dependency injection in KMP across platforms (based on Koin).</strong></li>
<li><strong>A brief background on On-Device Models and the Gemma 1's 2B LLM model.</strong></li>
</ol>
<p><img src="https://2bab-images.lastmayday.com/202408-on-device-model-screenshot.jpg?imageslim" alt=""></p>
<h2 id="on-device-model" tabindex="-1">On-Device Model</h2>
<p>Large Language Models (LLMs) have been a hot topic for quite some time, and this year, the trend has reached mobile devices. Companies like Google have deeply integrated on-device model functionalities into their latest smartphones and operating systems. Google's current public strategy regarding On-Device Models involves two main types of LLMs:</p>
<ol>
<li><strong>Gemini Nano:</strong> Not open source, limited support for specific devices (some are accelerated by specific chips like Tensor G4), and demonstrates extremely strong performance. It's currently available on desktop platforms (Chrome) and some Android phones (Pixel 8/9 and Samsung S23/24, etc.). It will be available to more developers later  for use and testing.</li>
<li><strong>Gemma:</strong> Open source and supports all devices that meet the minimum requirements, with impressive performance as well. It uses the similar technology as Nano for training with primarily-English data from web documents, mathematics, and code. It can be experienced on multiple platforms (Android/iOS/Desktop). The MediaPipe adaptation for Gemma2 will also be available soon according to the announcement.</li>
</ol>
<p>Since most developers do not yet have access to Gemini Nano, our focus today is on the 2B version of Gemma 1. To use Gemma directly on mobile platforms, Google has provided an out-of-the-box tool: MediaPipe. MediaPipe is a cross-platform framework that packages a series of pre-built on-device machine learning models and tools, supporting tasks like real-time gesture recognition, face detection, and more. It can also be used in applications like image generation and chatbots. For those interested, you can try the web version <a href="https://mediapipe-studio.webapps.google.com/">demo</a> and explore the relevant <a href="https://ai.google.dev/edge/mediapipe/solutions/guide">documentation</a>.</p>
<p><img src="https://2bab-images.lastmayday.com/on-device-model-mediapipe-intro.jpg?imageslim" alt=""></p>
<p>Among its features, the LLM Inference API (first row in the table above) is a component for running large language model inferences, supporting models like Gemma 2B/7B, Phi-2, Falcon-RW-1B, StableLM-3B, and more. Pre-converted models for Gemma (based on TensorFlow Lite) can be downloaded from Kaggle <a href="https://www.kaggle.com/models/google/gemma/tfLite/gemma-1.1-2b-it-gpu-int4">here</a> and loaded into MediaPipe later.</p>
<p><img src="https://2bab-images.lastmayday.com/202408-on-device-model-gemma-download.png?imageslim" alt=""></p>
<h2 id="llm-inference-android-sample" tabindex="-1">LLM Inference Android Sample</h2>
<p>The official <a href="https://github.com/google-ai-edge/mediapipe-samples/tree/main/examples/llm_inference/android">LLM Inference Demo</a> from MediaPipe includes support for Android, iOS, and Web platforms.</p>
<p><img src="https://2bab-images.lastmayday.com/202408-on-device-demo-android-sample.png?imageslim" alt=""></p>
<p>Opening the Android repository reveals several characteristics:</p>
<ul>
<li><strong>Pure Kotlin</strong> implementation.</li>
<li>The UI is implemented entirely with <strong>Jetpack Compose</strong>.</li>
<li>The LLM Task SDK it depends on is highly encapsulated, exposing only three methods.</li>
</ul>
<p>Now, let's check out the iOS version:</p>
<ul>
<li>The UI is implemented with SwiftUI, performing the same tasks as Compose but with some simplified elements (e.g., no top bar or send button).</li>
<li>The LLM Task SDK it relies on is also highly encapsulated, exposing the same three methods.</li>
</ul>
<p>This led to an interesting idea: <strong>The Android version has a foundation that allows it to be ported to iOS. Porting would make the code on both platforms highly consistent,  reducing maintenance costs, with the core implementation only requiring a bridge to the LLM Inference SDK on iOS.</strong></p>
<h2 id="kotlin-multiplatform" tabindex="-1">Kotlin Multiplatform</h2>
<p>The technology used for the porting project is called Kotlin Multiplatform (KMP), developed by the Kotlin team to support cross-platform development. KMP allows developers to use the same codebase to build applications for Android, iOS, Web, and other platforms. By sharing business logic code, KMP can significantly reduce development time and maintenance costs while preserving native performance and experience for each platform. At this yearâ€™s I/O conference, Google also announced first-class support for KMP, migrating some Android libraries and tools to the multiplatform, enabling KMP developers to use it conveniently on iOS and other platforms.</p>
<p><img src="https://2bab-images.lastmayday.com/202408-on-device-model-kmp-1.jpg?imageslim" alt="">
<img src="https://2bab-images.lastmayday.com/202408-on-device-model-kmp-2.jpg?imageslim" alt=""></p>
<p>Although MediaPipe supports multiple platforms, this time we mainly focus on Android and iOS.</p>
<h2 id="porting-process" tabindex="-1">Porting Process</h2>
<h3 id="initialization" tabindex="-1">Initialization</h3>
<p>Start by creating a basic KMP project using IntelliJ IDEA or Android Studio. You can use the KMP Wizard or templates from third-party KMP apps. If you're unfamiliar with KMP, you'll find its structure is quite similar to an Android project, except this time, we place the iOS container project in the root directory and configure iOS dependencies in the app module's <code>build.gradle.kts</code> with the KMP Gradle Plugin.</p>
<p><img src="https://2bab-images.lastmayday.com/202408-on-device-model-proj-3.jpg?imageslim" alt=""></p>
<h3 id="wrapping-and-calling-llm-inference" tabindex="-1">Wrapping and Calling LLM Inference</h3>
<p>In <code>commonMain</code>, we abstract a simple interface based on the characteristics of the MediaPipe LLM Task SDK, written in Kotlin to cater to both Android and iOS. This interface replaces the <code>InferenceModel.kt</code> class in the original repository.</p>
<pre class="language-kotlin"><code class="language-kotlin"><span class="token comment">// app/src/commonMain/.../llm/LLMOperator</span><br><span class="token keyword">interface</span> LLMOperator <span class="token punctuation">{</span><br><br>    <span class="token comment">/**<br>     * To load the model into the current context.<br>     * @return 1. null if successful, 2. an error message if failed.<br>     */</span><br>    <span class="token keyword">suspend</span> <span class="token keyword">fun</span> <span class="token function">initModel</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">:</span> String<span class="token operator">?</span><br><br>    <span class="token comment">/**<br>     * To calculate the token size of a string.<br>     */</span><br>    <span class="token keyword">fun</span> <span class="token function">sizeInTokens</span><span class="token punctuation">(</span>text<span class="token operator">:</span> String<span class="token punctuation">)</span><span class="token operator">:</span> Int<br><br>    <span class="token comment">/**<br>     * To generate response for a given inputText in a synchronous way.<br>     */</span><br>    <span class="token keyword">suspend</span> <span class="token keyword">fun</span> <span class="token function">generateResponse</span><span class="token punctuation">(</span>inputText<span class="token operator">:</span> String<span class="token punctuation">)</span><span class="token operator">:</span> String<br><br>    <span class="token comment">/**<br>     * To generate response for a given inputText in an asynchronous way.<br>     * @return A flow with partial response in String and completion flag in Boolean.<br>     */</span><br>    <span class="token keyword">suspend</span> <span class="token keyword">fun</span> <span class="token function">generateResponseAsync</span><span class="token punctuation">(</span>inputText<span class="token operator">:</span> String<span class="token punctuation">)</span><span class="token operator">:</span> Flow<span class="token operator">&lt;</span>Pair<span class="token operator">&lt;</span>String<span class="token punctuation">,</span> Boolean<span class="token operator">></span><span class="token operator">></span><br><br><span class="token punctuation">}</span></code></pre>
<p>On Android, since the LLM Task SDK was originally implemented in Kotlin, aside from initializing the model file, most of the functionality is essentially a proxy for the original SDK.</p>
<pre class="language-kotlin"><code class="language-kotlin"><span class="token keyword">class</span> <span class="token function">LLMInferenceAndroidImpl</span><span class="token punctuation">(</span><span class="token keyword">private</span> <span class="token keyword">val</span> ctx<span class="token operator">:</span> Context<span class="token punctuation">)</span><span class="token operator">:</span> LLMOperator <span class="token punctuation">{</span><br><br>    <span class="token keyword">private</span> <span class="token keyword">lateinit</span> <span class="token keyword">var</span> llmInference<span class="token operator">:</span> LlmInference<br>    <span class="token keyword">private</span> <span class="token keyword">val</span> initialized <span class="token operator">=</span> <span class="token function">AtomicBoolean</span><span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">)</span><br>    <span class="token keyword">private</span> <span class="token keyword">val</span> partialResultsFlow <span class="token operator">=</span> MutableSharedFlow<span class="token operator">&lt;</span>Pair<span class="token operator">&lt;</span>String<span class="token punctuation">,</span> Boolean<span class="token operator">></span><span class="token operator">></span><span class="token punctuation">(</span><span class="token operator">..</span><span class="token punctuation">.</span><span class="token punctuation">)</span><br><br>    <span class="token keyword">override</span> <span class="token keyword">suspend</span> <span class="token keyword">fun</span> <span class="token function">initModel</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">:</span> String<span class="token operator">?</span> <span class="token punctuation">{</span><br>        <span class="token keyword">if</span> <span class="token punctuation">(</span>initialized<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span><br>            <span class="token keyword">return</span> <span class="token keyword">null</span><br>        <span class="token punctuation">}</span><br>        <span class="token keyword">return</span> <span class="token keyword">try</span> <span class="token punctuation">{</span><br>            <span class="token keyword">val</span> modelPath <span class="token operator">=</span> <span class="token operator">..</span><span class="token punctuation">.</span><br>            <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">File</span><span class="token punctuation">(</span>modelPath<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">exists</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">not</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span><br>                <span class="token keyword">return</span> <span class="token string-literal singleline"><span class="token string">"Model not found at path: </span><span class="token interpolation"><span class="token interpolation-punctuation punctuation">$</span><span class="token expression">modelPath</span></span><span class="token string">"</span></span><br>            <span class="token punctuation">}</span><br>            <span class="token function">loadModel</span><span class="token punctuation">(</span>modelPath<span class="token punctuation">)</span><br>            initialized<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span><br>            <span class="token keyword">null</span><br>        <span class="token punctuation">}</span> <span class="token keyword">catch</span> <span class="token punctuation">(</span>e<span class="token operator">:</span> Exception<span class="token punctuation">)</span> <span class="token punctuation">{</span><br>            e<span class="token punctuation">.</span>message<br>        <span class="token punctuation">}</span><br>    <span class="token punctuation">}</span><br>    <span class="token keyword">private</span> <span class="token keyword">fun</span> <span class="token function">loadModel</span><span class="token punctuation">(</span>modelPath<span class="token operator">:</span> String<span class="token punctuation">)</span> <span class="token punctuation">{</span><br>        <span class="token keyword">val</span> options <span class="token operator">=</span> LlmInference<span class="token punctuation">.</span>LlmInferenceOptions<span class="token punctuation">.</span><span class="token function">builder</span><span class="token punctuation">(</span><span class="token punctuation">)</span><br>            <span class="token punctuation">.</span><span class="token function">setModelPath</span><span class="token punctuation">(</span>modelPath<span class="token punctuation">)</span><br>            <span class="token punctuation">.</span><span class="token function">setMaxTokens</span><span class="token punctuation">(</span><span class="token number">1024</span><span class="token punctuation">)</span><br>            <span class="token punctuation">.</span><span class="token function">setResultListener</span> <span class="token punctuation">{</span> partialResult<span class="token punctuation">,</span> done <span class="token operator">-></span><br>                <span class="token comment">// Transforming the listener to flow,</span><br>                <span class="token comment">// making it easy on UI integration.</span><br>                partialResultsFlow<span class="token punctuation">.</span><span class="token function">tryEmit</span><span class="token punctuation">(</span>partialResult <span class="token keyword">to</span> done<span class="token punctuation">)</span><br>            <span class="token punctuation">}</span><br>            <span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><br><br>        llmInference <span class="token operator">=</span> LlmInference<span class="token punctuation">.</span><span class="token function">createFromOptions</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> options<span class="token punctuation">)</span><br>    <span class="token punctuation">}</span><br><br>    <span class="token keyword">override</span> <span class="token keyword">fun</span> <span class="token function">sizeInTokens</span><span class="token punctuation">(</span>text<span class="token operator">:</span> String<span class="token punctuation">)</span><span class="token operator">:</span> Int <span class="token operator">=</span> llmInference<span class="token punctuation">.</span><span class="token function">sizeInTokens</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><br><br>    <span class="token keyword">override</span> <span class="token keyword">suspend</span> <span class="token keyword">fun</span> <span class="token function">generateResponse</span><span class="token punctuation">(</span>inputText<span class="token operator">:</span> String<span class="token punctuation">)</span><span class="token operator">:</span> String <span class="token punctuation">{</span><br>        <span class="token operator">..</span><span class="token punctuation">.</span><br>        <span class="token keyword">return</span> llmInference<span class="token punctuation">.</span><span class="token function">generateResponse</span><span class="token punctuation">(</span>inputText<span class="token punctuation">)</span><br>    <span class="token punctuation">}</span><br><br>    <span class="token keyword">override</span> <span class="token keyword">suspend</span> <span class="token keyword">fun</span> <span class="token function">generateResponseAsync</span><span class="token punctuation">(</span>inputText<span class="token operator">:</span> String<span class="token punctuation">)</span><span class="token operator">:</span> Flow<span class="token operator">&lt;</span>Pair<span class="token operator">&lt;</span>String<span class="token punctuation">,</span> Boolean<span class="token operator">></span><span class="token operator">></span> <span class="token punctuation">{</span><br>        <span class="token operator">..</span><span class="token punctuation">.</span><br>        llmInference<span class="token punctuation">.</span><span class="token function">generateResponseAsync</span><span class="token punctuation">(</span>inputText<span class="token punctuation">)</span><br>        <span class="token keyword">return</span> partialResultsFlow<span class="token punctuation">.</span><span class="token function">asSharedFlow</span><span class="token punctuation">(</span><span class="token punctuation">)</span><br>    <span class="token punctuation">}</span><br><br><span class="token punctuation">}</span></code></pre>
<p>For iOS, we first attempt the direct invocation of libraries added via Cocoapods. In the app module, include the Cocoapods plugin and add the MediaPipe LLM Task library:</p>
<pre class="language-kotlin"><code class="language-kotlin"><span class="token comment">// app/build.gradle.kts</span><br>plugins <span class="token punctuation">{</span><br>    <span class="token operator">..</span><span class="token punctuation">.</span><br>    <span class="token function">alias</span><span class="token punctuation">(</span>libs<span class="token punctuation">.</span>plugins<span class="token punctuation">.</span>cocoapods<span class="token punctuation">)</span><br><span class="token punctuation">}</span><br>cocoapods <span class="token punctuation">{</span><br>    <span class="token operator">..</span><span class="token punctuation">.</span><br>    ios<span class="token punctuation">.</span>deploymentTarget <span class="token operator">=</span> <span class="token string-literal singleline"><span class="token string">"15"</span></span><br><br>    <span class="token function">pod</span><span class="token punctuation">(</span><span class="token string-literal singleline"><span class="token string">"MediaPipeTasksGenAIC"</span></span><span class="token punctuation">)</span> <span class="token punctuation">{</span><br>        version <span class="token operator">=</span> <span class="token string-literal singleline"><span class="token string">"0.10.14"</span></span><br>        extraOpts <span class="token operator">+=</span> <span class="token function">listOf</span><span class="token punctuation">(</span><span class="token string-literal singleline"><span class="token string">"-compiler-option"</span></span><span class="token punctuation">,</span> <span class="token string-literal singleline"><span class="token string">"-fmodules"</span></span><span class="token punctuation">)</span><br>    <span class="token punctuation">}</span><br>    <span class="token function">pod</span><span class="token punctuation">(</span><span class="token string-literal singleline"><span class="token string">"MediaPipeTasksGenAI"</span></span><span class="token punctuation">)</span> <span class="token punctuation">{</span><br>        version <span class="token operator">=</span> <span class="token string-literal singleline"><span class="token string">"0.10.14"</span></span><br>        extraOpts <span class="token operator">+=</span> <span class="token function">listOf</span><span class="token punctuation">(</span><span class="token string-literal singleline"><span class="token string">"-compiler-option"</span></span><span class="token punctuation">,</span> <span class="token string-literal singleline"><span class="token string">"-fmodules"</span></span><span class="token punctuation">)</span><br>    <span class="token punctuation">}</span><br><span class="token punctuation">}</span></code></pre>
<p>Note the addition of the <code>-fmodules</code> compiler option in the above configuration to generate Kotlin references correctly (<a href="https://kotlinlang.org/docs/native-cocoapods-libraries.html#support-for-objective-c-headers-with-import-directives">reference link</a>).</p>
<blockquote>
<p>Some Objective-C libraries, specifically those that serve as wrappers for Swift libraries, have @import directives in their headers. By default, cinterop doesn't provide support for these directives. To enable support for @import directives, specify the -fmodules option in the configuration block of the pod() function.</p>
</blockquote>
<p>Afterward, in <code>iosMain</code>, you can directly import the relevant library code and replicate the Android proxy approach:</p>
<pre class="language-kotlin"><code class="language-kotlin"><span class="token comment">// Note these imports start with cocoapods</span><br><span class="token keyword">import</span> cocoapods<span class="token punctuation">.</span>MediaPipeTasksGenAI<span class="token punctuation">.</span>MPPLLMInference<br><span class="token keyword">import</span> cocoapods<span class="token punctuation">.</span>MediaPipeTasksGenAI<span class="token punctuation">.</span>MPPLLMInferenceOptions<br><span class="token keyword">import</span> platform<span class="token punctuation">.</span>Foundation<span class="token punctuation">.</span>NSBundle<br><span class="token operator">..</span><span class="token punctuation">.</span><br><span class="token keyword">class</span> LLMOperatorIOSImpl<span class="token operator">:</span> LLMOperator <span class="token punctuation">{</span><br><br>    <span class="token keyword">private</span> <span class="token keyword">val</span> inference<span class="token operator">:</span> MPPLLMInference<br>    <br>        <span class="token keyword">init</span> <span class="token punctuation">{</span><br>        <span class="token keyword">val</span> modelPath <span class="token operator">=</span> NSBundle<span class="token punctuation">.</span>mainBundle<span class="token punctuation">.</span><span class="token function">pathForResource</span><span class="token punctuation">(</span><span class="token operator">..</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token string-literal singleline"><span class="token string">"bin"</span></span><span class="token punctuation">)</span><br><br>        <span class="token keyword">val</span> options <span class="token operator">=</span> <span class="token function">MPPLLMInferenceOptions</span><span class="token punctuation">(</span>modelPath<span class="token operator">!!</span><span class="token punctuation">)</span><br>        options<span class="token punctuation">.</span><span class="token function">setModelPath</span><span class="token punctuation">(</span>modelPath<span class="token operator">!!</span><span class="token punctuation">)</span><br>        options<span class="token punctuation">.</span><span class="token function">setMaxTokens</span><span class="token punctuation">(</span><span class="token number">2048</span><span class="token punctuation">)</span><br>        options<span class="token punctuation">.</span><span class="token function">setTopk</span><span class="token punctuation">(</span><span class="token number">40</span><span class="token punctuation">)</span><br>        options<span class="token punctuation">.</span><span class="token function">setTemperature</span><span class="token punctuation">(</span><span class="token number">0.8f</span><span class="token punctuation">)</span><br>        options<span class="token punctuation">.</span><span class="token function">setRandomSeed</span><span class="token punctuation">(</span><span class="token number">102</span><span class="token punctuation">)</span><br><br>        <span class="token comment">// NPE was thrown here right after it printed the success initialization message internally.</span><br>        inference <span class="token operator">=</span> <span class="token function">MPPLLMInference</span><span class="token punctuation">(</span>options<span class="token punctuation">,</span> <span class="token keyword">null</span><span class="token punctuation">)</span> <br>    <span class="token punctuation">}</span><br><br>    <span class="token keyword">override</span> <span class="token keyword">fun</span> <span class="token function">generateResponse</span><span class="token punctuation">(</span>inputText<span class="token operator">:</span> String<span class="token punctuation">)</span><span class="token operator">:</span> String <span class="token punctuation">{</span><span class="token operator">..</span><span class="token punctuation">.</span><span class="token punctuation">}</span><br>    <span class="token keyword">override</span> <span class="token keyword">fun</span> <span class="token function">generateResponseAsync</span><span class="token punctuation">(</span>inputText<span class="token operator">:</span> String<span class="token punctuation">,</span> <span class="token operator">..</span><span class="token punctuation">.</span><span class="token punctuation">)</span> <span class="token operator">:</span><span class="token operator">..</span><span class="token punctuation">.</span> <span class="token punctuation">{</span><br>        <span class="token operator">..</span><span class="token punctuation">.</span><br>    <span class="token punctuation">}</span><br>    <span class="token operator">..</span><span class="token punctuation">.</span><br><span class="token punctuation">}</span></code></pre>
<p>However, we weren't as lucky this time. An NPE was thrown immediately after <code>MPPLLMInference</code> finished initializing. The likely issue is that since Kotlin's current interop target is Objective-C, the <code>MPPLLMInference</code> constructor has an extra error parameter compared to the Swift version, to which we passed <code>null</code>.</p>
<pre class="language-kotlin"><code class="language-kotlin"><span class="token keyword">constructor</span><span class="token punctuation">(</span><br>  options<span class="token operator">:</span> cocoapods<span class="token punctuation">.</span>MediaPipeTasksGenAI<span class="token punctuation">.</span>MPPLLMInferenceOptions<span class="token punctuation">,</span> <br>  error<span class="token operator">:</span> CPointer<span class="token operator">&lt;</span>ObjCObjectVar<span class="token operator">&lt;</span>platform<span class="token punctuation">.</span>Foundation<span class="token punctuation">.</span>NSError<span class="token operator">?</span><span class="token operator">></span><span class="token operator">></span><span class="token operator">?</span><span class="token punctuation">)</span></code></pre>
<p>Various attempts with different pointer inputs did not solve the problem:</p>
<pre class="language-kotlin"><code class="language-kotlin"><span class="token comment">// One of the attempts</span><br>memScoped <span class="token punctuation">{</span><br>    <span class="token keyword">val</span> pp<span class="token operator">:</span> CPointerVar<span class="token operator">&lt;</span>ObjCObjectVar<span class="token operator">&lt;</span>NSError<span class="token operator">?</span><span class="token operator">></span><span class="token operator">></span> <span class="token operator">=</span> <span class="token function">allocPointerTo</span><span class="token punctuation">(</span><span class="token punctuation">)</span><br>    <span class="token keyword">val</span> inference <span class="token operator">=</span> <span class="token function">MPPLLMInference</span><span class="token punctuation">(</span>options<span class="token punctuation">,</span> pp<span class="token punctuation">.</span>value<span class="token punctuation">)</span><br>    Napier<span class="token punctuation">.</span><span class="token function">i</span><span class="token punctuation">(</span>pp<span class="token punctuation">.</span>value<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br><span class="token punctuation">}</span></code></pre>
<p>Thus, we had to adopt a different approach: calling the third-party library from the iOS project.</p>
<pre class="language-kotlin"><code class="language-kotlin"><span class="token comment">// 1. Declare an interface similar to LLMOperator for easier iOS SDK adaptation.</span><br><span class="token comment">// app/src/iosMain/.../llm/LLMOperator.kt</span><br><span class="token keyword">interface</span> LLMOperatorSwift <span class="token punctuation">{</span><br>    <span class="token keyword">suspend</span> <span class="token keyword">fun</span> <span class="token function">loadModel</span><span class="token punctuation">(</span>modelName<span class="token operator">:</span> String<span class="token punctuation">)</span><br>    <span class="token keyword">fun</span> <span class="token function">sizeInTokens</span><span class="token punctuation">(</span>text<span class="token operator">:</span> String<span class="token punctuation">)</span><span class="token operator">:</span> Int<br>    <span class="token keyword">suspend</span> <span class="token keyword">fun</span> <span class="token function">generateResponse</span><span class="token punctuation">(</span>inputText<span class="token operator">:</span> String<span class="token punctuation">)</span><span class="token operator">:</span> String<br>    <span class="token keyword">suspend</span> <span class="token keyword">fun</span> <span class="token function">generateResponseAsync</span><span class="token punctuation">(</span><br>        inputText<span class="token operator">:</span> String<span class="token punctuation">,</span><br>        progress<span class="token operator">:</span> <span class="token punctuation">(</span>partialResponse<span class="token operator">:</span> String<span class="token punctuation">)</span> <span class="token operator">-></span> Unit<span class="token punctuation">,</span><br>        completion<span class="token operator">:</span> <span class="token punctuation">(</span>completeResponse<span class="token operator">:</span> String<span class="token punctuation">)</span> <span class="token operator">-></span> Unit<br>    <span class="token punctuation">)</span><br><span class="token punctuation">}</span><br><br><span class="token comment">// 2. Implement this interface in the iOS project</span><br><span class="token comment">// iosApp/iosApp/LLMInferenceDelegate.swift</span><br><span class="token keyword">class</span> LLMOperatorSwiftImpl<span class="token operator">:</span> LLMOperatorSwift <span class="token punctuation">{</span><br>    <span class="token operator">..</span><span class="token punctuation">.</span><br>    <span class="token keyword">var</span> llmInference<span class="token operator">:</span> LlmInference<span class="token operator">?</span><br>    <br>    func <span class="token function">loadModel</span><span class="token punctuation">(</span>modelName<span class="token operator">:</span> String<span class="token punctuation">)</span> async throws <span class="token punctuation">{</span><br>        let path <span class="token operator">=</span> Bundle<span class="token punctuation">.</span>main<span class="token punctuation">.</span><span class="token function">path</span><span class="token punctuation">(</span>forResource<span class="token operator">:</span> modelName<span class="token punctuation">,</span> ofType<span class="token operator">:</span> <span class="token string-literal singleline"><span class="token string">"bin"</span></span><span class="token punctuation">)</span><span class="token operator">!</span><br>        let llmOptions <span class="token operator">=</span>  LlmInference<span class="token punctuation">.</span><span class="token function">Options</span><span class="token punctuation">(</span>modelPath<span class="token operator">:</span> path<span class="token punctuation">)</span><br>        llmOptions<span class="token punctuation">.</span>maxTokens <span class="token operator">=</span> <span class="token number">4096</span><br>        llmOptions<span class="token punctuation">.</span>temperature <span class="token operator">=</span> <span class="token number">0.9</span><br>        <br>        llmInference <span class="token operator">=</span> <span class="token keyword">try</span> <span class="token function">LlmInference</span><span class="token punctuation">(</span>options<span class="token operator">:</span> llmOptions<span class="token punctuation">)</span><br>    <span class="token punctuation">}</span><br>    <br>    func <span class="token function">generateResponse</span><span class="token punctuation">(</span>inputText<span class="token operator">:</span> String<span class="token punctuation">)</span> async throws <span class="token operator">-></span> String <span class="token punctuation">{</span><br>        <span class="token keyword">return</span> <span class="token keyword">try</span> llmInference<span class="token operator">!</span><span class="token punctuation">.</span><span class="token function">generateResponse</span><span class="token punctuation">(</span>inputText<span class="token operator">:</span> inputText<span class="token punctuation">)</span><br>    <span class="token punctuation">}</span><br>    <br>    func <span class="token function">generateResponseAsync</span><span class="token punctuation">(</span>inputText<span class="token operator">:</span> String<span class="token punctuation">,</span> progress<span class="token operator">:</span> <span class="token label symbol">@escaping</span> <span class="token punctuation">(</span>String<span class="token punctuation">)</span> <span class="token operator">-></span> Void<span class="token punctuation">,</span> completion<span class="token operator">:</span> <span class="token label symbol">@escaping</span> <span class="token punctuation">(</span>String<span class="token punctuation">)</span> <span class="token operator">-></span> Void<span class="token punctuation">)</span> async throws <span class="token punctuation">{</span><br>        <span class="token keyword">try</span> llmInference<span class="token operator">!</span><span class="token punctuation">.</span><span class="token function">generateResponseAsync</span><span class="token punctuation">(</span>inputText<span class="token operator">:</span> inputText<span class="token punctuation">)</span> <span class="token punctuation">{</span> partialResponse<span class="token punctuation">,</span> error <span class="token keyword">in</span><br>            <span class="token comment">// progress</span><br>            <span class="token keyword">if</span> let e <span class="token operator">=</span> error <span class="token punctuation">{</span><br>                <span class="token function">print</span><span class="token punctuation">(</span><span class="token string-literal singleline"><span class="token string">"\(self.errorTag) \(e)"</span></span><span class="token punctuation">)</span><br>                <span class="token function">completion</span><span class="token punctuation">(</span>e<span class="token punctuation">.</span>localizedDescription<span class="token punctuation">)</span><br>                <span class="token keyword">return</span><br>            <span class="token punctuation">}</span><br>            <span class="token keyword">if</span> let partial <span class="token operator">=</span> partialResponse <span class="token punctuation">{</span><br>                <span class="token function">progress</span><span class="token punctuation">(</span>partial<span class="token punctuation">)</span><br>            <span class="token punctuation">}</span><br>        <span class="token punctuation">}</span> completion<span class="token operator">:</span> <span class="token punctuation">{</span><br>            <span class="token function">completion</span><span class="token punctuation">(</span><span class="token string-literal singleline"><span class="token string">""</span></span><span class="token punctuation">)</span><br>        <span class="token punctuation">}</span><br>    <span class="token punctuation">}</span><br>    <span class="token operator">..</span><span class="token punctuation">.</span>    <br><span class="token punctuation">}</span><br><br><span class="token comment">// 3. iOS then passes back the delegated (initialization-focused) object to Kotlin</span><br><span class="token comment">// iosApp/iosApp/iosApp.swift</span><br><span class="token keyword">class</span> AppDelegate<span class="token operator">:</span> UIResponder<span class="token punctuation">,</span> UIApplicationDelegate <span class="token punctuation">{</span><br>    <span class="token operator">..</span><span class="token punctuation">.</span><br>    func <span class="token function">application</span><span class="token punctuation">(</span>ï¼‰<span class="token punctuation">{</span><br>        <span class="token operator">..</span><span class="token punctuation">.</span><br>        let delegate <span class="token operator">=</span> <span class="token keyword">try</span> <span class="token function">LLMOperatorSwiftImpl</span><span class="token punctuation">(</span><span class="token punctuation">)</span><br>        MainKt<span class="token punctuation">.</span><span class="token function">onStartup</span><span class="token punctuation">(</span>llmInferenceDelegate<span class="token operator">:</span> delegate<span class="token punctuation">)</span>        <br>    <span class="token punctuation">}</span><br><span class="token punctuation">}</span><br><br><span class="token comment">// 4. The initial implementation object for iOS in KMP </span><br><span class="token comment">//  are directly delegated to it (injected via constructor)</span><br><span class="token keyword">class</span> <span class="token function">LLMOperatorIOSImpl</span><span class="token punctuation">(</span><br>   <span class="token keyword">private</span> <span class="token keyword">val</span> delegate<span class="token operator">:</span> LLMOperatorSwift<span class="token punctuation">)</span> <span class="token operator">:</span> LLMOperator <span class="token punctuation">{</span>   <br>   <span class="token operator">..</span><span class="token punctuation">.</span><br><span class="token punctuation">}</span></code></pre>
<p>You might notice that the Impl instances on both platforms require different constructor parameters. This issue is generally resolved using KMP's <code>expect</code> and <code>actual</code> keywords. In the following code:</p>
<ol>
<li>We take advantage of the fact that the <code>expect</code> class does not require constructor parameters, adding a layer of encapsulation (similar to an interface).</li>
<li>We use <a href="https://insert-koin.io/">Koin</a> to inject the necessary parameters for each platform, then uniformly inject the created interface instance into the Common layer as needed.</li>
</ol>
<pre class="language-kotlin"><code class="language-kotlin"><span class="token comment">// Common</span><br><span class="token keyword">expect</span> <span class="token keyword">class</span> LLMOperatorFactory <span class="token punctuation">{</span><br>    <span class="token keyword">fun</span> <span class="token function">create</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">:</span> LLMOperator<br><span class="token punctuation">}</span><br><span class="token keyword">val</span> sharedModule <span class="token operator">=</span> module <span class="token punctuation">{</span><br>   <span class="token comment">// Create the LLMOperator required by the Common layer from different LLMOperatorFactory implementations</span><br>	single<span class="token operator">&lt;</span>LLMOperator<span class="token operator">></span> <span class="token punctuation">{</span> <span class="token keyword">get</span><span class="token operator">&lt;</span>LLMOperatorFactory<span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">create</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">}</span><br><span class="token punctuation">}</span><br><br><span class="token comment">// Android</span><br><span class="token keyword">actual</span> <span class="token keyword">class</span> <span class="token function">LLMOperatorFactory</span><span class="token punctuation">(</span><span class="token keyword">private</span> <span class="token keyword">val</span> context<span class="token operator">:</span> Context<span class="token punctuation">)</span><span class="token punctuation">{</span><br>    <span class="token keyword">actual</span> <span class="token keyword">fun</span> <span class="token function">create</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">:</span> LLMOperator <span class="token operator">=</span> <span class="token function">LLMInferenceAndroidImpl</span><span class="token punctuation">(</span>context<span class="token punctuation">)</span><br><span class="token punctuation">}</span><br><span class="token keyword">val</span> androidModule <span class="token operator">=</span> module <span class="token punctuation">{</span><br>    <span class="token comment">// Android injects the App's Context</span><br>    single <span class="token punctuation">{</span> <span class="token function">LLMOperatorFactory</span><span class="token punctuation">(</span><span class="token function">androidContext</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">}</span><br><span class="token punctuation">}</span><br><br><span class="token comment">// iOS</span><br><span class="token keyword">actual</span> <span class="token keyword">class</span> <span class="token function">LLMOperatorFactory</span><span class="token punctuation">(</span><span class="token keyword">private</span> <span class="token keyword">val</span> llmInferenceDelegate<span class="token operator">:</span> LLMOperatorSwift<span class="token punctuation">)</span> <span class="token punctuation">{</span><br>    <span class="token keyword">actual</span> <span class="token keyword">fun</span> <span class="token function">create</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">:</span> LLMOperator <span class="token operator">=</span> <span class="token function">LLMOperatorIOSImpl</span><span class="token punctuation">(</span>llmInferenceDelegate<span class="token punctuation">)</span><br><span class="token punctuation">}</span><br><br>module <span class="token punctuation">{</span><br>    <span class="token comment">// iOS injects the delegate passed in the onStartup function</span><br>    single <span class="token punctuation">{</span> <span class="token function">LLMOperatorFactory</span><span class="token punctuation">(</span>llmInferenceDelegate<span class="token punctuation">)</span> <span class="token punctuation">}</span><br><span class="token punctuation">}</span></code></pre>
<p>In summary, this case study gave us a taste of <strong>deep interaction between Kotlin and Swift</strong>. By leveraging the <code>expect</code> and <code>actual</code> keywords along with Koin's dependency injection, we made the overall solution <strong>smoother and more automated</strong>, achieving the goal of calling Android and iOS native SDKs from the Common module in KMP.</p>
<h3 id="porting-ui-and-viewmodel" tabindex="-1">Porting UI and ViewModel</h3>
<p>The <code>InferenceMode</code> in the original project has been replaced by the <code>LLMOperator</code> from the previous section, so we copy the remaining five classes excluding Activity:</p>
<p><img src="https://2bab-images.lastmayday.com/202408-on-device-model-copy-structure.png?imageslim" alt=""></p>
<p>Next, we make a few modifications to allow Jetpack Compose code to migrate easily to Compose Multiplatform.</p>
<p>First, the <code>ViewModel</code>. In the KMP version, I used <a href="https://github.com/adrielcafe/voyager">Voyage</a>, replacing it with <code>ScreenModel</code>. While an official ViewModel solution is also in the works, which you can refer to in this <a href="https://www.jetbrains.com/help/kotlin-multiplatform-dev/compose-viewmodel.html">document</a>.</p>
<pre class="language-kotlin"><code class="language-kotlin"><span class="token comment">// Android version</span><br><span class="token keyword">class</span> <span class="token function">ChatViewModel</span><span class="token punctuation">(</span><br>    <span class="token keyword">private</span> <span class="token keyword">val</span> inferenceModel<span class="token operator">:</span> InferenceModel<br><span class="token punctuation">)</span> <span class="token operator">:</span> <span class="token function">ViewModel</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token operator">..</span><span class="token punctuation">.</span><span class="token punctuation">}</span><br><br><span class="token comment">// KMP version, converts ViewModel to ScreenModel and modifies the input object</span><br><span class="token keyword">class</span> <span class="token function">ChatViewModel</span><span class="token punctuation">(</span><br>    <span class="token keyword">private</span> <span class="token keyword">val</span> llmOperator<span class="token operator">:</span> LLMOperator<br><span class="token punctuation">)</span> <span class="token operator">:</span> ScreenModel <span class="token punctuation">{</span><span class="token operator">..</span><span class="token punctuation">.</span><span class="token punctuation">}</span></code></pre>
<p>Correspondingly, the ViewModel initialization method is also changed to the ScreenModel method:</p>
<pre class="language-kotlin"><code class="language-kotlin"><span class="token comment">// Android version</span><br><span class="token annotation builtin">@Composable</span><br><span class="token keyword">internal</span> <span class="token keyword">fun</span> <span class="token function">ChatRoute</span><span class="token punctuation">(</span><br>    chatViewModel<span class="token operator">:</span> ChatViewModel <span class="token operator">=</span> <span class="token function">viewModel</span><span class="token punctuation">(</span><br>        factory <span class="token operator">=</span> ChatViewModel<span class="token punctuation">.</span><span class="token function">getFactory</span><span class="token punctuation">(</span>LocalContext<span class="token punctuation">.</span>current<span class="token punctuation">.</span>applicationContext<span class="token punctuation">)</span><br>    <span class="token punctuation">)</span><br><span class="token punctuation">)</span> <span class="token punctuation">{</span><br>    <span class="token operator">..</span><span class="token punctuation">.</span><br>    <span class="token function">ChatScreen</span><span class="token punctuation">(</span><span class="token operator">..</span><span class="token punctuation">.</span><span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token operator">..</span><span class="token punctuation">.</span><span class="token punctuation">}</span><br><span class="token punctuation">}</span><br><br><span class="token comment">// KMP version, initialized externally and passed in</span><br><span class="token annotation builtin">@Composable</span><br><span class="token keyword">internal</span> <span class="token keyword">fun</span> <span class="token function">ChatRoute</span><span class="token punctuation">(</span><br>    chatViewModel<span class="token operator">:</span> ChatViewModel<br><span class="token punctuation">)</span> <span class="token punctuation">{</span><br><br><span class="token comment">// Here we use the default parameter injection solution for decoupling.</span><br><span class="token comment">// koinInject() is a method provided by Koin for @Composable function injection in Compose.</span><br><span class="token annotation builtin">@Composable</span><br><span class="token keyword">fun</span> <span class="token function">AiScreen</span><span class="token punctuation">(</span>llmOperator<span class="token operator">:</span>LLMOperator <span class="token operator">=</span> <span class="token function">koinInject</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span><br>    <span class="token comment">// Use the remember method from ScreenModel</span><br>    <span class="token keyword">val</span><br><br> chatViewModel <span class="token operator">=</span> rememberScreenModel <span class="token punctuation">{</span> <span class="token function">ChatViewModel</span><span class="token punctuation">(</span>llmOperator<span class="token punctuation">)</span> <span class="token punctuation">}</span><br>    <span class="token operator">..</span><span class="token punctuation">.</span><br>    Column <span class="token punctuation">{</span><br>        <span class="token operator">..</span><span class="token punctuation">.</span><br>        <span class="token function">Box</span><span class="token punctuation">(</span><span class="token operator">..</span><span class="token punctuation">.</span><span class="token punctuation">)</span> <span class="token punctuation">{</span><br>            <span class="token keyword">if</span> <span class="token punctuation">(</span>showLoading<span class="token punctuation">)</span> <span class="token punctuation">{</span><br>                <span class="token operator">..</span><span class="token punctuation">.</span><br>            <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span><br>                <span class="token function">ChatRoute</span><span class="token punctuation">(</span>chatViewModel<span class="token punctuation">)</span><br>            <span class="token punctuation">}</span><br>        <span class="token punctuation">}</span><br>    <span class="token punctuation">}</span><br><span class="token punctuation">}</span></code></pre>
<p>The corresponding LLM functionality calls within the ViewModel also need to be replaced:</p>
<pre class="language-kotlin"><code class="language-kotlin"><span class="token comment">// Android version</span><br>inferenceModel<span class="token punctuation">.</span><span class="token function">generateResponseAsync</span><span class="token punctuation">(</span>fullPrompt<span class="token punctuation">)</span><br>inferenceModel<span class="token punctuation">.</span>partialResults<br>    <span class="token punctuation">.</span><span class="token function">collectIndexed</span> <span class="token punctuation">{</span> index<span class="token punctuation">,</span> <span class="token punctuation">(</span>partialResult<span class="token punctuation">,</span> done<span class="token punctuation">)</span> <span class="token operator">-></span><br>        <span class="token operator">..</span><span class="token punctuation">.</span><br>    <span class="token punctuation">}</span><br><br><span class="token comment">// KMP version, moves Flow's return to the front, compatible with SDK design on both platforms</span><br>llmOperator<span class="token punctuation">.</span><span class="token function">generateResponseAsync</span><span class="token punctuation">(</span>fullPrompt<span class="token punctuation">)</span><br>    <span class="token punctuation">.</span><span class="token function">collectIndexed</span> <span class="token punctuation">{</span> index<span class="token punctuation">,</span> <span class="token punctuation">(</span>partialResult<span class="token punctuation">,</span> done<span class="token punctuation">)</span> <span class="token operator">-></span><br>        <span class="token operator">..</span><span class="token punctuation">.</span><br>    <span class="token punctuation">}</span></code></pre>
<p>Next, adapt resource loading methods specific to Compose Multiplatform, replacing <code>R</code> with <code>Res</code>:</p>
<pre class="language-kotlin"><code class="language-kotlin"><span class="token comment">// Android version</span><br><span class="token function">Text</span><span class="token punctuation">(</span><span class="token function">stringResource</span><span class="token punctuation">(</span>R<span class="token punctuation">.</span>string<span class="token punctuation">.</span>chat_label<span class="token punctuation">)</span><span class="token punctuation">)</span><br><br><span class="token comment">// KMP version, this reference is mapped from xml by the plugin</span><br><span class="token comment">// (commonMain/composeResources/values/strings.xml)</span><br><span class="token keyword">import</span> mediapiper<span class="token punctuation">.</span>app<span class="token punctuation">.</span>generated<span class="token punctuation">.</span>resources<span class="token punctuation">.</span>chat_label<br><span class="token operator">..</span><span class="token punctuation">.</span><br><span class="token function">Text</span><span class="token punctuation">(</span><span class="token function">stringResource</span><span class="token punctuation">(</span>Res<span class="token punctuation">.</span>string<span class="token punctuation">.</span>chat_label<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre>
<p>At this point, we have completed the main UI and functionality migration of <code>ChatScreen</code> and <code>ChatViewModel</code>.</p>
<p>Finally, there are a few minor modifications:</p>
<ul>
<li>For <code>LoadingScreen</code>, we replicate the approach of passing in <code>LLMOperator</code> for initialization (replacing the original <code>InferenceModel</code>).</li>
<li><code>ChatMessage</code> requires only a single line API change for UUID (which won't be needed after Kotlin 2.0.20).</li>
<li><code>ChatUiState</code> does not require any changes.</li>
<li>The remaining minor tweaks include changing log library and other small details.</li>
</ul>
<p>In summary, setting aside log and R file replacing, <strong>the core changes less than 20 lines</strong>, allowing the <strong>entire UI to function as expected</strong>.</p>
<h2 id="simple-testing" tabindex="-1">Simple Testing</h2>
<p>So, how does the performance of Gemma 2B measure up? Let's look at some simple examples with Pixel 4a and iOS emulators. Here, we primarily test three versions of the model, defined in <code>me.xx2bab.mediapiper.llm.LLMOperator</code> (refer to the project README for deploying models on both platforms):</p>
<ul>
<li><code>gemma-2b-it-gpu-int4</code></li>
<li><code>gemma-2b-it-cpu-int4</code></li>
<li><code>gemma-2b-it-cpu-int8</code></li>
</ul>
<p>Key points to note:</p>
<ul>
<li><strong>it</strong> indicates an Instruction Tuned variant, better suited for conversational use because they are fine-tuned to better understand instructions and generate more relevant responses.</li>
<li><strong>int4/8</strong> refers to model quantization, which converts floating-point numbers in the model to lower-precision integers, thereby reducing the model's size and computation, making it suitable for small local devices like phones. However, the model's precision and response accuracy might decrease.</li>
<li><strong>cpu</strong> and <strong>gpu</strong> refer to the target hardware platforms, allowing devices with weak or no GPUs to choose CPU execution. From the test results below, you'll find that CPU versions often outperform on current mobile devices, due to the small model size, simple computation (dialogue), and integer quantization favor CPU instruction execution.</li>
</ul>
<p>First, we test a simple logic: &quot;Is asparagus an animal?&quot; As shown in the image below, the CPU version provides a more reasonable answer than both GPU versions (iOS and Android). The next test is translating the answer into Chinese, which doesn't perform well across all three attempts, but this is expected.</p>
<p><img src="https://2bab-images.lastmayday.com/202408-on-device-model-test-1.jpg?imageslim" alt=""></p>
<p>Next, we elevate the complexity of the question to word classification between animals and plants: both GPU and CPU versions perform well.</p>
<p><img src="https://2bab-images.lastmayday.com/202408-on-device-model-test-2.jpg?imageslim" alt=""></p>
<p>Raising the complexity further, asking it to output the answer in JSON format, leads to apparent issues:</p>
<ol>
<li>The first image does not output a complete snippet, missing the closing three dots ```.</li>
<li>The second image shows a classification error, placing mangosteen under animals, and sunflowers appear twice under plants.</li>
<li>The third image mirrors the second error, and none of the three instances strictly outputs a JSON, failing to rigorously follow the role of a JSON Responder.</li>
</ol>
<p><img src="https://2bab-images.lastmayday.com/202408-on-device-model-test-3.jpg?imageslim" alt=""></p>
<p>Lastly, this isn't the limit. Using the cpu-int8 version can answer the above questions with higher accuracy. Moreover, if you send the entry code for this demo's iOS version for analysis, it performs quite well.</p>
<p><img src="https://2bab-images.lastmayday.com/202408-on-device-model-test-4.jpg?imageslim" alt=""></p>
<p>Testing the Gemma 1's 2B version reveals that its inference capabilities still have room for improvement, but it excels in response speed. In fact, the 2B version of Gemma 2 was released recently, and according to official tests, its overall performance has surpassed GPT 3.5. This means that on a small mobile phone, local inference can now achieve the results of mainstream models from a year and a half ago. However, it has yet to be adapted to TFLite (on which MediaPipe is based).It's on the roadmap but without a specific date, you can track the following issues for the latest updates:</p>
<ul>
<li><a href="https://github.com/google-ai-edge/mediapipe/issues/5570">https://github.com/google-ai-edge/mediapipe/issues/5570</a></li>
<li><a href="https://github.com/google-ai-edge/mediapipe/issues/5594">https://github.com/google-ai-edge/mediapipe/issues/5594</a></li>
</ul>
<h2 id="conclusion" tabindex="-1">Conclusion</h2>
<p>Migrating this local chat demo and conducting tests provided us with some firsthand experience:</p>
<ul>
<li>The development of On-Device Models for LLMs is progressing rapidly. With the help of Google's infrastructure, third-party mobile app developers can quickly integrate related features across Android and iOS platforms.</li>
<li>Considering the current situation, On-Device Models for LLMs are likely to reach a preliminary usable state this year. Inference speed is already good, but accuracy still requires further testing (e.g., Gemma 2's 2B version + MediaPipe).</li>
<li>Adopting the &quot;Kotlin First&quot; strategy and boldly using Compose shows great promiseâ€”under a well-developed infrastructure, a small chat module of Android can be ported to iOS with just a handful of changes.</li>
</ul>

    </div>
    <hr />
      <div class="text-sm text-center">
        For comments and further discussion, mail to xx2bab@gmail.com
      </div>
      <hr />
    
    <footer class="text-sm py-12 text-gray-500 text-center">
  
  <p><a href="/">ENG</a> / <a href="/zh">ä¸­æ–‡</a></p>
  
  2BAB's Blog since 2014
</footer>
  </div>

</div>



</body>
</html>